{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(max_depth=2, learning_rate=0.01, n_estimators=1000)\n",
    "\n",
    "#p = cross_val_predict(model, train[[f]].values, train['target'], method='predict_proba')[:, 1]\n",
    "#'%.4f' % roc_auc_score(train['target'], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "counters = np.load('value_counters.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(200):\n",
    "    counters[i] = pd.Series(counters[i]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 0 ns, total: 1.24 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(200):\n",
    "    f = 'var_%d' % i\n",
    "    f_vc = '%s_vc' % f\n",
    "\n",
    "    train[f_vc] = counters[i][train[f]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 0 ns, total: 1.28 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(200):\n",
    "    f = 'var_%d' % i\n",
    "    f_vc = '%s_vc' % f\n",
    "\n",
    "    test[f_vc] = counters[i][test[f]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5217'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = cross_val_predict(model, train[[f, '%s_vc' % f]].values, train['target'], method='predict_proba')[:, 1]\n",
    "'%.4f' % roc_auc_score(train['target'], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#for i in range(200):\n",
    "#    f = 'var_%d' % i\n",
    "#    train[f] = train[f].rank()\n",
    "    \n",
    "a, b = train_test_split(train, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(200)\n",
    "results = [[] for i in features]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_leaves\n",
    "[(11, 114), (23, 23), (5, 21), (29, 16), (41, 12)]\n",
    "subsample_freq\n",
    "[(2, 105), (5, 35), (8, 14), (1, 11), (9, 11)]\n",
    "learning_rate\n",
    "[(0.033, 114), (0.09000000000000001, 23), (0.027, 12), (0.032, 11), (0.057999999999999996, 8)]\n",
    "n_estimators\n",
    "[(100, 135), (300, 25), (200, 16), (400, 14), (500, 8)]\n",
    "subsample\n",
    "[(0.5, 126), (0.8999999999999999, 33), (0.6, 15), (0.4, 11), (0.7999999999999999, 8)]\n",
    "random_state\n",
    "[(0, 200)]\n",
    "max_depth\n",
    "[(2, 200)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.03, 'n_estimators': 110, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "3:\tLL 424\tAUC 52\n",
      "8:\tLL 446\tAUC 196\n",
      "9:\tLL 587\tAUC 452\n",
      "12:\tLL 803\tAUC 637\n",
      "44:\tLL 648\tAUC 393\n",
      "49:\tLL 518\tAUC 266\n",
      "86:\tLL 604\tAUC 367\n",
      "97:\tLL 459\tAUC 168\n",
      "99:\tLL 660\tAUC 499\n",
      "110:\tLL 713\tAUC 513\n",
      "118:\tLL 548\tAUC 456\n",
      "120:\tLL 423\tAUC 109\n",
      "123:\tLL 598\tAUC 446\n",
      "135:\tLL 520\tAUC 273\n",
      "145:\tLL 493\tAUC 234\n",
      "170:\tLL 593\tAUC 350\n",
      "192:\tLL 505\tAUC 401\n",
      "0.52618, 0.32485\n",
      "\n",
      "0.92270\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.045, 'n_estimators': 10, 'subsample': 0.5, 'random_state': 0, 'max_depth': 1}\n",
      "14:\tLL 415\tAUC 18\n",
      "25:\tLL 417\tAUC 21\n",
      "63:\tLL 418\tAUC 133\n",
      "65:\tLL 418\tAUC 104\n",
      "93:\tLL 462\tAUC 351\n",
      "96:\tLL 413\tAUC 20\n",
      "140:\tLL 420\tAUC 139\n",
      "143:\tLL 418\tAUC 65\n",
      "152:\tLL 418\tAUC 76\n",
      "0.52617, 0.32485\n",
      "\n",
      "0.92277\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 2, 'learning_rate': 0.06, 'n_estimators': 400, 'subsample': 0.5, 'random_state': 0, 'max_depth': 1}\n",
      "53:\tLL 692\tAUC 525\n",
      "129:\tLL 418\tAUC 96\n",
      "0.52617, 0.32485\n",
      "\n",
      "0.92277\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.04, 'n_estimators': 10, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "44:\tLL 560\tAUC 422\n",
      "52:\tLL 449\tAUC 274\n",
      "56:\tLL 478\tAUC 249\n",
      "132:\tLL 441\tAUC 200\n",
      "152:\tLL 423\tAUC 77\n",
      "161:\tLL 416\tAUC 56\n",
      "176:\tLL 411\tAUC 31\n",
      "187:\tLL 446\tAUC 202\n",
      "193:\tLL 429\tAUC 179\n",
      "0.52618, 0.32485\n",
      "\n",
      "0.92277\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.07, 'n_estimators': 280, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "66:\tLL 450\tAUC 243\n",
      "68:\tLL 409\tAUC 126\n",
      "177:\tLL 571\tAUC 350\n",
      "0.52618, 0.32485\n",
      "\n",
      "0.92279\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.08, 'n_estimators': 290, 'subsample': 0.5, 'random_state': 0, 'max_depth': 1}\n",
      "38:\tLL 419\tAUC 104\n",
      "55:\tLL 430\tAUC 149\n",
      "129:\tLL 420\tAUC 99\n",
      "0.52618, 0.32485\n",
      "\n",
      "0.92279\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 3, 'learning_rate': 0.1, 'n_estimators': 320, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "38:\tLL 419\tAUC 109\n",
      "53:\tLL 694\tAUC 508\n",
      "108:\tLL 594\tAUC 430\n",
      "110:\tLL 680\tAUC 547\n",
      "129:\tLL 418\tAUC 104\n",
      "0.52618, 0.32485\n",
      "\n",
      "0.92279\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.095, 'n_estimators': 110, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "26:\tLL 763\tAUC 535\n",
      "30:\tLL 413\tAUC 68\n",
      "90:\tLL 485\tAUC 254\n",
      "98:\tLL 413\tAUC 109\n",
      "120:\tLL 421\tAUC 126\n",
      "130:\tLL 528\tAUC 293\n",
      "135:\tLL 524\tAUC 307\n",
      "146:\tLL 737\tAUC 575\n",
      "171:\tLL 441\tAUC 143\n",
      "174:\tLL 760\tAUC 536\n",
      "179:\tLL 714\tAUC 465\n",
      "180:\tLL 516\tAUC 323\n",
      "0.52623, 0.32485\n",
      "\n",
      "0.92290\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.045, 'n_estimators': 150, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "54:\tLL 427\tAUC 142\n",
      "62:\tLL 433\tAUC 163\n",
      "0.52623, 0.32485\n",
      "\n",
      "0.92290\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 4, 'learning_rate': 0.045, 'n_estimators': 180, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "49:\tLL 521\tAUC 268\n",
      "131:\tLL 487\tAUC 297\n",
      "137:\tLL 527\tAUC 315\n",
      "169:\tLL 606\tAUC 516\n",
      "190:\tLL 670\tAUC 494\n",
      "0.52623, 0.32485\n",
      "\n",
      "0.92292\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 3, 'learning_rate': 0.03, 'n_estimators': 100, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "34:\tLL 581\tAUC 487\n",
      "39:\tLL 415\tAUC 50\n",
      "65:\tLL 415\tAUC 124\n",
      "71:\tLL 500\tAUC 376\n",
      "91:\tLL 514\tAUC 349\n",
      "95:\tLL 500\tAUC 311\n",
      "98:\tLL 420\tAUC 119\n",
      "159:\tLL 434\tAUC 202\n",
      "183:\tLL 418\tAUC 58\n",
      "0.52624, 0.32485\n",
      "\n",
      "0.92292\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 1, 'learning_rate': 0.03, 'n_estimators': 190, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "32:\tLL 453\tAUC 198\n",
      "33:\tLL 657\tAUC 402\n",
      "58:\tLL 475\tAUC 311\n",
      "120:\tLL 423\tAUC 110\n",
      "122:\tLL 593\tAUC 431\n",
      "128:\tLL 497\tAUC 310\n",
      "195:\tLL 496\tAUC 321\n",
      "197:\tLL 532\tAUC 378\n",
      "0.52624, 0.32485\n",
      "\n",
      "0.92292\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 4, 'learning_rate': 0.06, 'n_estimators': 50, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "57:\tLL 430\tAUC 128\n",
      "75:\tLL 503\tAUC 367\n",
      "0.52624, 0.32485\n",
      "\n",
      "0.92292\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.03, 'n_estimators': 130, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "98:\tLL 421\tAUC 96\n",
      "106:\tLL 473\tAUC 304\n",
      "128:\tLL 485\tAUC 331\n",
      "185:\tLL 412\tAUC 11\n",
      "0.52624, 0.32485\n",
      "\n",
      "0.92292\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 1, 'learning_rate': 0.04, 'n_estimators': 220, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "1:\tLL 581\tAUC 400\n",
      "5:\tLL 555\tAUC 320\n",
      "13:\tLL 663\tAUC 513\n",
      "80:\tLL 745\tAUC 591\n",
      "122:\tLL 595\tAUC 435\n",
      "135:\tLL 523\tAUC 308\n",
      "166:\tLL 635\tAUC 538\n",
      "169:\tLL 606\tAUC 524\n",
      "184:\tLL 630\tAUC 489\n",
      "0.52626, 0.32485\n",
      "\n",
      "0.92298\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.065, 'n_estimators': 190, 'subsample': 0.7, 'random_state': 0, 'max_depth': 2}\n",
      "2:\tLL 723\tAUC 544\n",
      "9:\tLL 598\tAUC 454\n",
      "18:\tLL 632\tAUC 364\n",
      "47:\tLL 433\tAUC 149\n",
      "115:\tLL 547\tAUC 423\n",
      "0.52626, 0.32485\n",
      "\n",
      "0.92299\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 1, 'learning_rate': 0.05500000000000001, 'n_estimators': 120, 'subsample': 1.0, 'random_state': 0, 'max_depth': 2}\n",
      "30:\tLL 416\tAUC 70\n",
      "53:\tLL 697\tAUC 495\n",
      "55:\tLL 459\tAUC 134\n",
      "61:\tLL 431\tAUC 62\n",
      "64:\tLL 431\tAUC 123\n",
      "70:\tLL 522\tAUC 334\n",
      "74:\tLL 440\tAUC 151\n",
      "86:\tLL 603\tAUC 387\n",
      "92:\tLL 638\tAUC 428\n",
      "102:\tLL 454\tAUC 184\n",
      "113:\tLL 437\tAUC 46\n",
      "120:\tLL 424\tAUC 109\n",
      "121:\tLL 526\tAUC 290\n",
      "128:\tLL 498\tAUC 331\n",
      "160:\tLL 424\tAUC 66\n",
      "193:\tLL 458\tAUC 184\n",
      "195:\tLL 496\tAUC 314\n",
      "0.52631, 0.32485\n",
      "\n",
      "0.92305\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.08, 'n_estimators': 300, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "10:\tLL 405\tAUC 81\n",
      "16:\tLL 437\tAUC 176\n",
      "41:\tLL 406\tAUC 71\n",
      "180:\tLL 516\tAUC 307\n",
      "0.52631, 0.32485\n",
      "\n",
      "0.92306\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 3, 'learning_rate': 0.09000000000000001, 'n_estimators': 310, 'subsample': 0.8, 'random_state': 0, 'max_depth': 1}\n",
      "90:\tLL 459\tAUC 268\n",
      "0.52631, 0.32485\n",
      "\n",
      "0.92306\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 3, 'learning_rate': 0.065, 'n_estimators': 240, 'subsample': 0.7, 'random_state': 0, 'max_depth': 1}\n",
      "169:\tLL 603\tAUC 532\n",
      "0.52631, 0.32485\n",
      "\n",
      "0.92306\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 1, 'learning_rate': 0.05500000000000001, 'n_estimators': 120, 'subsample': 0.5, 'random_state': 0, 'max_depth': 1}\n",
      "8:\tLL 441\tAUC 201\n",
      "43:\tLL 468\tAUC 254\n",
      "69:\tLL 433\tAUC 84\n",
      "0.52632, 0.32485\n",
      "\n",
      "0.92306\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.095, 'n_estimators': 260, 'subsample': 0.2, 'random_state': 0, 'max_depth': 1}\n",
      "167:\tLL 446\tAUC 270\n",
      "0.52632, 0.32485\n",
      "\n",
      "0.92306\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 1, 'learning_rate': 0.05500000000000001, 'n_estimators': 40, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "3:\tLL 423\tAUC 61\n",
      "8:\tLL 448\tAUC 192\n",
      "35:\tLL 467\tAUC 300\n",
      "54:\tLL 428\tAUC 141\n",
      "55:\tLL 459\tAUC 123\n",
      "63:\tLL 433\tAUC 93\n",
      "75:\tLL 569\tAUC 374\n",
      "87:\tLL 500\tAUC 295\n",
      "107:\tLL 563\tAUC 404\n",
      "116:\tLL 445\tAUC 206\n",
      "120:\tLL 424\tAUC 105\n",
      "121:\tLL 523\tAUC 328\n",
      "150:\tLL 468\tAUC 184\n",
      "152:\tLL 427\tAUC 39\n",
      "154:\tLL 604\tAUC 433\n",
      "160:\tLL 425\tAUC 58\n",
      "162:\tLL 537\tAUC 389\n",
      "164:\tLL 612\tAUC 332\n",
      "175:\tLL 452\tAUC 166\n",
      "186:\tLL 479\tAUC 303\n",
      "198:\tLL 617\tAUC 412\n",
      "0.52633, 0.32484\n",
      "\n",
      "0.92329\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.075, 'n_estimators': 150, 'subsample': 0.8, 'random_state': 0, 'max_depth': 1}\n",
      "62:\tLL 433\tAUC 167\n",
      "0.52633, 0.32484\n",
      "\n",
      "0.92329\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.045, 'n_estimators': 80, 'subsample': 0.2, 'random_state': 0, 'max_depth': 2}\n",
      "7:\tLL 414\tAUC 77\n",
      "9:\tLL 587\tAUC 458\n",
      "18:\tLL 630\tAUC 368\n",
      "19:\tLL 461\tAUC 219\n",
      "22:\tLL 706\tAUC 506\n",
      "48:\tLL 514\tAUC 375\n",
      "56:\tLL 531\tAUC 221\n",
      "70:\tLL 522\tAUC 340\n",
      "81:\tLL 878\tAUC 656\n",
      "83:\tLL 473\tAUC 241\n",
      "84:\tLL 409\tAUC 56\n",
      "87:\tLL 501\tAUC 273\n",
      "89:\tLL 547\tAUC 357\n",
      "99:\tLL 658\tAUC 526\n",
      "107:\tLL 582\tAUC 402\n",
      "136:\tLL 412\tAUC 70\n",
      "141:\tLL 568\tAUC 334\n",
      "149:\tLL 609\tAUC 583\n",
      "151:\tLL 493\tAUC 247\n",
      "156:\tLL 444\tAUC 191\n",
      "167:\tLL 478\tAUC 271\n",
      "177:\tLL 579\tAUC 333\n",
      "0.52641, 0.32484\n",
      "\n",
      "0.92354\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 1, 'learning_rate': 0.05, 'n_estimators': 150, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "26:\tLL 766\tAUC 525\n",
      "83:\tLL 472\tAUC 242\n",
      "167:\tLL 484\tAUC 257\n",
      "0.52642, 0.32484\n",
      "\n",
      "0.92355\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 4, 'learning_rate': 0.09000000000000001, 'n_estimators': 300, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "0.52642, 0.32484\n",
      "\n",
      "0.92355\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 1, 'learning_rate': 0.095, 'n_estimators': 10, 'subsample': 0.7, 'random_state': 0, 'max_depth': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35:\tLL 471\tAUC 312\n",
      "44:\tLL 629\tAUC 429\n",
      "45:\tLL 459\tAUC 268\n",
      "48:\tLL 488\tAUC 380\n",
      "54:\tLL 428\tAUC 133\n",
      "56:\tLL 512\tAUC 260\n",
      "59:\tLL 423\tAUC 41\n",
      "141:\tLL 538\tAUC 347\n",
      "152:\tLL 428\tAUC 55\n",
      "161:\tLL 415\tAUC 67\n",
      "194:\tLL 435\tAUC 125\n",
      "0.52642, 0.32484\n",
      "\n",
      "0.92357\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 3, 'learning_rate': 0.095, 'n_estimators': 180, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "0.52642, 0.32484\n",
      "\n",
      "0.92357\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 2, 'learning_rate': 0.035, 'n_estimators': 350, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "129:\tLL 417\tAUC 105\n",
      "0.52642, 0.32484\n",
      "\n",
      "0.92357\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.05, 'n_estimators': 60, 'subsample': 1.0, 'random_state': 0, 'max_depth': 2}\n",
      "6:\tLL 712\tAUC 620\n",
      "53:\tLL 698\tAUC 517\n",
      "55:\tLL 462\tAUC 138\n",
      "61:\tLL 431\tAUC 69\n",
      "63:\tLL 435\tAUC 103\n",
      "66:\tLL 451\tAUC 225\n",
      "82:\tLL 522\tAUC 287\n",
      "106:\tLL 482\tAUC 295\n",
      "108:\tLL 594\tAUC 425\n",
      "110:\tLL 714\tAUC 515\n",
      "114:\tLL 467\tAUC 207\n",
      "121:\tLL 527\tAUC 326\n",
      "123:\tLL 589\tAUC 454\n",
      "141:\tLL 568\tAUC 336\n",
      "160:\tLL 425\tAUC 69\n",
      "164:\tLL 613\tAUC 346\n",
      "175:\tLL 454\tAUC 165\n",
      "186:\tLL 481\tAUC 301\n",
      "0.52645, 0.32484\n",
      "\n",
      "0.92368\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 1, 'learning_rate': 0.07, 'n_estimators': 340, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "0.52645, 0.32484\n",
      "\n",
      "0.92368\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.05500000000000001, 'n_estimators': 220, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "58:\tLL 477\tAUC 306\n",
      "66:\tLL 449\tAUC 246\n",
      "139:\tLL 943\tAUC 781\n",
      "178:\tLL 474\tAUC 257\n",
      "182:\tLL 433\tAUC 132\n",
      "199:\tLL 506\tAUC 357\n",
      "0.52645, 0.32484\n",
      "\n",
      "0.92371\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.085, 'n_estimators': 260, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "73:\tLL 424\tAUC 91\n",
      "0.52646, 0.32484\n",
      "\n",
      "0.92372\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 3, 'learning_rate': 0.05500000000000001, 'n_estimators': 110, 'subsample': 0.7, 'random_state': 0, 'max_depth': 1}\n",
      "12:\tLL 808\tAUC 611\n",
      "189:\tLL 427\tAUC 162\n",
      "190:\tLL 620\tAUC 520\n",
      "0.52647, 0.32484\n",
      "\n",
      "0.92374\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.09000000000000001, 'n_estimators': 160, 'subsample': 0.5, 'random_state': 0, 'max_depth': 1}\n",
      "79:\tLL 428\tAUC 79\n",
      "110:\tLL 680\tAUC 548\n",
      "0.52647, 0.32484\n",
      "\n",
      "0.92375\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 4, 'learning_rate': 0.05500000000000001, 'n_estimators': 170, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "5:\tLL 557\tAUC 345\n",
      "13:\tLL 659\tAUC 520\n",
      "113:\tLL 432\tAUC 105\n",
      "127:\tLL 524\tAUC 374\n",
      "144:\tLL 454\tAUC 185\n",
      "159:\tLL 438\tAUC 205\n",
      "190:\tLL 670\tAUC 490\n",
      "197:\tLL 535\tAUC 386\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92382\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 1, 'learning_rate': 0.025, 'n_estimators': 170, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "28:\tLL 447\tAUC 229\n",
      "34:\tLL 582\tAUC 495\n",
      "41:\tLL 418\tAUC 52\n",
      "72:\tLL 435\tAUC 200\n",
      "125:\tLL 494\tAUC 289\n",
      "177:\tLL 567\tAUC 358\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92381\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 3, 'learning_rate': 0.095, 'n_estimators': 100, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "43:\tLL 465\tAUC 255\n",
      "57:\tLL 430\tAUC 123\n",
      "131:\tLL 480\tAUC 305\n",
      "148:\tLL 650\tAUC 511\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92382\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.05500000000000001, 'n_estimators': 280, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "38:\tLL 418\tAUC 111\n",
      "59:\tLL 424\tAUC 46\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92383\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 2, 'learning_rate': 0.035, 'n_estimators': 210, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "41:\tLL 418\tAUC 55\n",
      "195:\tLL 497\tAUC 293\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92385\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 2, 'learning_rate': 0.085, 'n_estimators': 230, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92385\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 4, 'learning_rate': 0.04, 'n_estimators': 380, 'subsample': 0.8, 'random_state': 0, 'max_depth': 2}\n",
      "0.52650, 0.32484\n",
      "\n",
      "0.92385\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.085, 'n_estimators': 70, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "3:\tLL 422\tAUC 64\n",
      "11:\tLL 470\tAUC 238\n",
      "12:\tLL 808\tAUC 626\n",
      "38:\tLL 418\tAUC 112\n",
      "71:\tLL 503\tAUC 376\n",
      "77:\tLL 445\tAUC 184\n",
      "141:\tLL 572\tAUC 316\n",
      "143:\tLL 420\tAUC 78\n",
      "151:\tLL 494\tAUC 249\n",
      "179:\tLL 724\tAUC 447\n",
      "188:\tLL 551\tAUC 304\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92387\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.06, 'n_estimators': 260, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "24:\tLL 524\tAUC 395\n",
      "81:\tLL 872\tAUC 675\n",
      "108:\tLL 588\tAUC 454\n",
      "139:\tLL 951\tAUC 785\n",
      "165:\tLL 661\tAUC 523\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92384\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 2, 'learning_rate': 0.095, 'n_estimators': 390, 'subsample': 0.8, 'random_state': 0, 'max_depth': 2}\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92384\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 4, 'learning_rate': 0.085, 'n_estimators': 300, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "0:\tLL 643\tAUC 487\n",
      "108:\tLL 586\tAUC 474\n",
      "153:\tLL 390\tAUC 26\n",
      "176:\tLL 392\tAUC 109\n",
      "185:\tLL 390\tAUC 23\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92384\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 4, 'learning_rate': 0.08, 'n_estimators': 170, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92384\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 1, 'learning_rate': 0.095, 'n_estimators': 300, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "38:\tLL 419\tAUC 117\n",
      "0.52654, 0.32484\n",
      "\n",
      "0.92385\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 1, 'learning_rate': 0.025, 'n_estimators': 20, 'subsample': 0.7, 'random_state': 0, 'max_depth': 1}\n",
      "4:\tLL 419\tAUC 68\n",
      "14:\tLL 415\tAUC 28\n",
      "29:\tLL 414\tAUC 4\n",
      "103:\tLL 415\tAUC 2\n",
      "153:\tLL 415\tAUC 12\n",
      "0.52657, 0.32484\n",
      "\n",
      "0.92386\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.075, 'n_estimators': 230, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "68:\tLL 410\tAUC 128\n",
      "138:\tLL 483\tAUC 281\n",
      "0.52656, 0.32484\n",
      "\n",
      "0.92387\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 1, 'learning_rate': 0.03, 'n_estimators': 140, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "36:\tLL 551\tAUC 432\n",
      "67:\tLL 558\tAUC 417\n",
      "99:\tLL 661\tAUC 514\n",
      "108:\tLL 595\tAUC 451\n",
      "122:\tLL 594\tAUC 437\n",
      "123:\tLL 599\tAUC 445\n",
      "145:\tLL 493\tAUC 224\n",
      "169:\tLL 602\tAUC 533\n",
      "0.52656, 0.32484\n",
      "\n",
      "0.92383\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 1, 'learning_rate': 0.025, 'n_estimators': 140, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "6:\tLL 713\tAUC 627\n",
      "23:\tLL 446\tAUC 254\n",
      "52:\tLL 484\tAUC 256\n",
      "54:\tLL 428\tAUC 137\n",
      "113:\tLL 437\tAUC 49\n",
      "120:\tLL 425\tAUC 117\n",
      "122:\tLL 594\tAUC 441\n",
      "155:\tLL 555\tAUC 320\n",
      "160:\tLL 426\tAUC 79\n",
      "164:\tLL 610\tAUC 347\n",
      "0.52657, 0.32484\n",
      "\n",
      "0.92379\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 4, 'learning_rate': 0.065, 'n_estimators': 340, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "0.52657, 0.32484\n",
      "\n",
      "0.92379\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 3, 'learning_rate': 0.08, 'n_estimators': 210, 'subsample': 0.8, 'random_state': 0, 'max_depth': 2}\n",
      "0.52657, 0.32484\n",
      "\n",
      "0.92379\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 1, 'learning_rate': 0.05500000000000001, 'n_estimators': 340, 'subsample': 0.2, 'random_state': 0, 'max_depth': 1}\n",
      "99:\tLL 650\tAUC 528\n",
      "0.52657, 0.32484\n",
      "\n",
      "0.92379\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 3, 'learning_rate': 0.04, 'n_estimators': 330, 'subsample': 0.2, 'random_state': 0, 'max_depth': 2}\n",
      "0:\tLL 656\tAUC 487\n",
      "18:\tLL 635\tAUC 371\n",
      "46:\tLL 418\tAUC 87\n",
      "51:\tLL 546\tAUC 287\n",
      "102:\tLL 449\tAUC 185\n",
      "131:\tLL 488\tAUC 319\n",
      "160:\tLL 423\tAUC 105\n",
      "191:\tLL 627\tAUC 453\n",
      "195:\tLL 504\tAUC 339\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 1, 'learning_rate': 0.065, 'n_estimators': 310, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.045, 'n_estimators': 280, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "145:\tLL 469\tAUC 235\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 4, 'learning_rate': 0.02, 'n_estimators': 220, 'subsample': 0.7, 'random_state': 0, 'max_depth': 2}\n",
      "6:\tLL 712\tAUC 644\n",
      "73:\tLL 426\tAUC 36\n",
      "88:\tLL 473\tAUC 237\n",
      "119:\tLL 508\tAUC 275\n",
      "163:\tLL 553\tAUC 326\n",
      "199:\tLL 506\tAUC 360\n",
      "0.52662, 0.32484\n",
      "\n",
      "0.92392\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.04, 'n_estimators': 150, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "98:\tLL 421\tAUC 107\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92392\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 4, 'learning_rate': 0.025, 'n_estimators': 250, 'subsample': 0.2, 'random_state': 0, 'max_depth': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\tLL 448\tAUC 197\n",
      "69:\tLL 432\tAUC 120\n",
      "72:\tLL 434\tAUC 210\n",
      "91:\tLL 508\tAUC 361\n",
      "98:\tLL 421\tAUC 113\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.03, 'n_estimators': 250, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.02, 'n_estimators': 280, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "15:\tLL 448\tAUC 199\n",
      "34:\tLL 583\tAUC 477\n",
      "54:\tLL 429\tAUC 135\n",
      "85:\tLL 465\tAUC 215\n",
      "0.52661, 0.32484\n",
      "\n",
      "0.92393\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 2, 'learning_rate': 0.06, 'n_estimators': 110, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "12:\tLL 802\tAUC 643\n",
      "30:\tLL 416\tAUC 75\n",
      "32:\tLL 446\tAUC 201\n",
      "41:\tLL 419\tAUC 80\n",
      "98:\tLL 423\tAUC 90\n",
      "111:\tLL 485\tAUC 284\n",
      "189:\tLL 427\tAUC 178\n",
      "190:\tLL 619\tAUC 524\n",
      "0.52662, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 2, 'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.8, 'random_state': 0, 'max_depth': 2}\n",
      "0.52662, 0.32484\n",
      "\n",
      "0.92395\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.07, 'n_estimators': 140, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "28:\tLL 447\tAUC 244\n",
      "46:\tLL 417\tAUC 91\n",
      "50:\tLL 429\tAUC 172\n",
      "60:\tLL 432\tAUC 108\n",
      "97:\tLL 461\tAUC 184\n",
      "167:\tLL 486\tAUC 268\n",
      "176:\tLL 408\tAUC 127\n",
      "190:\tLL 674\tAUC 505\n",
      "0.52666, 0.32484\n",
      "\n",
      "0.92409\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.09000000000000001, 'n_estimators': 50, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "0.52666, 0.32484\n",
      "\n",
      "0.92409\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 1, 'learning_rate': 0.085, 'n_estimators': 170, 'subsample': 1.0, 'random_state': 0, 'max_depth': 2}\n",
      "199:\tLL 513\tAUC 357\n",
      "0.52666, 0.32484\n",
      "\n",
      "0.92410\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.06, 'n_estimators': 10, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "17:\tLL 416\tAUC 53\n",
      "25:\tLL 419\tAUC 60\n",
      "63:\tLL 430\tAUC 134\n",
      "65:\tLL 421\tAUC 119\n",
      "103:\tLL 415\tAUC 30\n",
      "104:\tLL 445\tAUC 280\n",
      "105:\tLL 437\tAUC 199\n",
      "116:\tLL 440\tAUC 211\n",
      "124:\tLL 416\tAUC 62\n",
      "152:\tLL 423\tAUC 84\n",
      "183:\tLL 418\tAUC 82\n",
      "185:\tLL 415\tAUC 19\n",
      "194:\tLL 436\tAUC 133\n",
      "0.52667, 0.32484\n",
      "\n",
      "0.92414\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.04, 'n_estimators': 330, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "0.52667, 0.32484\n",
      "\n",
      "0.92414\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.09000000000000001, 'n_estimators': 110, 'subsample': 1.0, 'random_state': 0, 'max_depth': 1}\n",
      "0.52667, 0.32484\n",
      "\n",
      "0.92414\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 1, 'learning_rate': 0.09000000000000001, 'n_estimators': 170, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "5:\tLL 559\tAUC 336\n",
      "0.52667, 0.32484\n",
      "\n",
      "0.92415\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.02, 'n_estimators': 180, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "36:\tLL 551\tAUC 439\n",
      "40:\tLL 684\tAUC 492\n",
      "54:\tLL 429\tAUC 143\n",
      "56:\tLL 531\tAUC 209\n",
      "74:\tLL 436\tAUC 168\n",
      "94:\tLL 595\tAUC 407\n",
      "99:\tLL 662\tAUC 508\n",
      "110:\tLL 715\tAUC 528\n",
      "129:\tLL 427\tAUC 119\n",
      "151:\tLL 490\tAUC 255\n",
      "186:\tLL 482\tAUC 304\n",
      "194:\tLL 430\tAUC 140\n",
      "0.52668, 0.32484\n",
      "\n",
      "0.92430\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.05, 'n_estimators': 140, 'subsample': 0.2, 'random_state': 0, 'max_depth': 1}\n",
      "71:\tLL 503\tAUC 379\n",
      "85:\tLL 463\tAUC 241\n",
      "111:\tLL 488\tAUC 267\n",
      "0.52669, 0.32484\n",
      "\n",
      "0.92429\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 2, 'learning_rate': 0.03, 'n_estimators': 150, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "9:\tLL 597\tAUC 466\n",
      "40:\tLL 684\tAUC 500\n",
      "88:\tLL 474\tAUC 238\n",
      "118:\tLL 551\tAUC 453\n",
      "123:\tLL 605\tAUC 451\n",
      "180:\tLL 517\tAUC 282\n",
      "0.52670, 0.32484\n",
      "\n",
      "0.92433\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 1, 'learning_rate': 0.05500000000000001, 'n_estimators': 40, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "0.52670, 0.32484\n",
      "\n",
      "0.92433\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 2, 'learning_rate': 0.1, 'n_estimators': 110, 'subsample': 0.8, 'random_state': 0, 'max_depth': 2}\n",
      "174:\tLL 760\tAUC 544\n",
      "0.52670, 0.32484\n",
      "\n",
      "0.92433\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 1, 'learning_rate': 0.085, 'n_estimators': 220, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "42:\tLL 427\tAUC 145\n",
      "0.52670, 0.32484\n",
      "\n",
      "0.92433\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 2, 'learning_rate': 0.025, 'n_estimators': 380, 'subsample': 0.2, 'random_state': 0, 'max_depth': 1}\n",
      "69:\tLL 434\tAUC 71\n",
      "0.52670, 0.32484\n",
      "\n",
      "0.92433\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.085, 'n_estimators': 160, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "3:\tLL 414\tAUC 68\n",
      "77:\tLL 448\tAUC 215\n",
      "0.52671, 0.32484\n",
      "\n",
      "0.92435\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 4, 'learning_rate': 0.085, 'n_estimators': 170, 'subsample': 1.0, 'random_state': 0, 'max_depth': 2}\n",
      "0.52671, 0.32484\n",
      "\n",
      "0.92435\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.05, 'n_estimators': 160, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "0.52671, 0.32484\n",
      "\n",
      "0.92435\n",
      "\n",
      "{'num_leaves': 11, 'subsample_freq': 3, 'learning_rate': 0.035, 'n_estimators': 60, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "8:\tLL 441\tAUC 201\n",
      "14:\tLL 413\tAUC 56\n",
      "15:\tLL 445\tAUC 243\n",
      "39:\tLL 415\tAUC 54\n",
      "68:\tLL 421\tAUC 92\n",
      "133:\tLL 601\tAUC 457\n",
      "0.52672, 0.32484\n",
      "\n",
      "0.92435\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 2, 'learning_rate': 0.075, 'n_estimators': 20, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n",
      "59:\tLL 425\tAUC 40\n",
      "103:\tLL 416\tAUC 40\n",
      "171:\tLL 425\tAUC 156\n",
      "0.52673, 0.32484\n",
      "\n",
      "0.92436\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 1, 'learning_rate': 0.095, 'n_estimators': 380, 'subsample': 0.6000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "0.52673, 0.32484\n",
      "\n",
      "0.92436\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 2, 'learning_rate': 0.04, 'n_estimators': 120, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "41:\tLL 418\tAUC 88\n",
      "130:\tLL 529\tAUC 267\n",
      "147:\tLL 636\tAUC 393\n",
      "159:\tLL 440\tAUC 202\n",
      "180:\tLL 518\tAUC 305\n",
      "0.52674, 0.32484\n",
      "\n",
      "0.92438\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 4, 'learning_rate': 0.045, 'n_estimators': 340, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "10:\tLL 416\tAUC 70\n",
      "54:\tLL 429\tAUC 152\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.075, 'n_estimators': 330, 'subsample': 0.8, 'random_state': 0, 'max_depth': 1}\n",
      "90:\tLL 458\tAUC 271\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.075, 'n_estimators': 360, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 1}\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 7, 'subsample_freq': 3, 'learning_rate': 0.085, 'n_estimators': 270, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.07, 'n_estimators': 310, 'subsample': 0.5, 'random_state': 0, 'max_depth': 2}\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.025, 'n_estimators': 50, 'subsample': 0.8, 'random_state': 0, 'max_depth': 1}\n",
      "4:\tLL 420\tAUC 72\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.05500000000000001, 'n_estimators': 360, 'subsample': 0.30000000000000004, 'random_state': 0, 'max_depth': 2}\n",
      "179:\tLL 712\tAUC 465\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 4, 'learning_rate': 0.045, 'n_estimators': 290, 'subsample': 0.8, 'random_state': 0, 'max_depth': 1}\n",
      "79:\tLL 430\tAUC 96\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 15, 'subsample_freq': 3, 'learning_rate': 0.04, 'n_estimators': 390, 'subsample': 0.7, 'random_state': 0, 'max_depth': 1}\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92439\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 3, 'learning_rate': 0.05500000000000001, 'n_estimators': 250, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 2}\n",
      "138:\tLL 482\tAUC 296\n",
      "147:\tLL 636\tAUC 397\n",
      "0.52676, 0.32484\n",
      "\n",
      "0.92440\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 3, 'learning_rate': 0.08, 'n_estimators': 100, 'subsample': 0.4, 'random_state': 0, 'max_depth': 2}\n",
      "1:\tLL 579\tAUC 416\n",
      "3:\tLL 423\tAUC 73\n",
      "5:\tLL 563\tAUC 335\n",
      "20:\tLL 469\tAUC 250\n",
      "38:\tLL 419\tAUC 124\n",
      "39:\tLL 411\tAUC 61\n",
      "107:\tLL 583\tAUC 392\n",
      "135:\tLL 525\tAUC 294\n",
      "137:\tLL 521\tAUC 326\n",
      "182:\tLL 434\tAUC 112\n",
      "0.52678, 0.32484\n",
      "\n",
      "0.92446\n",
      "\n",
      "{'num_leaves': 9, 'subsample_freq': 4, 'learning_rate': 0.04, 'n_estimators': 50, 'subsample': 0.4, 'random_state': 0, 'max_depth': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68:\tLL 422\tAUC 80\n",
      "126:\tLL 417\tAUC 102\n",
      "128:\tLL 482\tAUC 332\n",
      "157:\tLL 505\tAUC 346\n",
      "0.52679, 0.32484\n",
      "\n",
      "0.92446\n",
      "\n",
      "{'num_leaves': 13, 'subsample_freq': 2, 'learning_rate': 0.085, 'n_estimators': 360, 'subsample': 0.9000000000000001, 'random_state': 0, 'max_depth': 1}\n",
      "38:\tLL 420\tAUC 118\n",
      "0.52678, 0.32484\n",
      "\n",
      "0.92446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "for iteration in range(100):\n",
    "    parameters = {\n",
    "        'max_depth': random.choice([1, 2]), \n",
    "        'learning_rate': random.choice(np.linspace(0.02, 0.1, 17)), \n",
    "        'n_estimators': random.choice(range(10, 410, 10)),\n",
    "        'subsample': random.choice(np.linspace(0.2, 1.0, 9)),\n",
    "        'subsample_freq': random.choice(range(1, 5)),\n",
    "        'num_leaves': random.choice(range(7, 17, 2)),\n",
    "        'random_state': 0,\n",
    "    }\n",
    "    print parameters\n",
    "    model = LGBMClassifier(**parameters)\n",
    "\n",
    "    for i in features:\n",
    "        f = 'var_%d' % i\n",
    "        f_vc = '%s_vc' % f\n",
    "        model.fit(a[[f, f_vc]].values, a['target'])\n",
    "\n",
    "        p = model.predict_proba(b[[f, f_vc]].values)[:, 1]\n",
    "        score = roc_auc_score(b['target'], p)\n",
    "        score2 = log_loss(b['target'], p)\n",
    "\n",
    "        if score > max(results[i])[0] or score2 < min_log_loss(results[i])[1]:\n",
    "            results[i].append((score, score2, parameters, p))\n",
    "            print '%d:\\tLL %d\\tAUC %d' % (i, 100000 * (0.33 - score2), 10000 * abs(score - 0.5))\n",
    "            \n",
    "    p = select_p(results, min_log_loss_n)\n",
    "    p = aggregate(p)\n",
    "    t = b['target'].values\n",
    "    \n",
    "    print\n",
    "    print '%.5f' % roc_auc_score(t, p)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dca4347f094dd389a6017b4477c496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.77672978, 2.97180211, 7.40064691, ..., 0.09006194, 1.63697561,\n",
       "       1.73380037])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_test(results, opt_f):\n",
    "    predictions = []\n",
    "    for i in tqdm.tqdm_notebook(range(200)):\n",
    "        f = 'var_%d' % i\n",
    "        f_vc = '%s_vc' % f\n",
    "        \n",
    "        _, _, parameters, _ = opt_f(results[i])\n",
    "        fixed_parameters = dict(parameters)\n",
    "        fixed_parameters['n_estimators'] = int(1.1 * fixed_parameters['n_estimators'])\n",
    "        \n",
    "        model = LGBMClassifier(**fixed_parameters)\n",
    "        \n",
    "        model.fit(train[[f, f_vc]].values, train['target'])\n",
    "        p = model.predict_proba(test[[f, f_vc]].values)[:, 1]\n",
    "        \n",
    "        predictions.append(p)\n",
    "        \n",
    "    predictions = np.column_stack(predictions)    \n",
    "    return predictions\n",
    "\n",
    "p_test = predict_test(results, min_log_loss)\n",
    "p_test = aggregate(p_test)\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1689ee028c4cb0b25cd2e4c815e478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.32393\n",
      "0, 0.54606\n",
      "\n",
      "1, 0.32402\n",
      "1, 0.57220\n",
      "\n",
      "2, 0.32361\n",
      "2, 0.59451\n",
      "\n",
      "3, 0.32605\n",
      "3, 0.59568\n",
      "\n",
      "4, 0.32607\n",
      "4, 0.59655\n",
      "\n",
      "5, 0.32457\n",
      "5, 0.60566\n",
      "\n",
      "6, 0.32327\n",
      "6, 0.62637\n",
      "\n",
      "7, 0.32617\n",
      "7, 0.62626\n",
      "\n",
      "8, 0.32584\n",
      "8, 0.62827\n",
      "\n",
      "9, 0.32449\n",
      "9, 0.63854\n",
      "\n",
      "10, 0.32614\n",
      "10, 0.63872\n",
      "\n",
      "11, 0.32571\n",
      "11, 0.64107\n",
      "\n",
      "12, 0.32265\n",
      "12, 0.65807\n",
      "\n",
      "13, 0.32360\n",
      "13, 0.66901\n",
      "\n",
      "14, 0.32614\n",
      "14, 0.66915\n",
      "\n",
      "15, 0.32596\n",
      "15, 0.67008\n",
      "\n",
      "16, 0.32619\n",
      "16, 0.66977\n",
      "\n",
      "17, 0.32617\n",
      "17, 0.66976\n",
      "\n",
      "18, 0.32432\n",
      "18, 0.67716\n",
      "\n",
      "19, 0.32598\n",
      "19, 0.67770\n",
      "\n",
      "20, 0.32588\n",
      "20, 0.67912\n",
      "\n",
      "21, 0.32346\n",
      "21, 0.68923\n",
      "\n",
      "22, 0.32330\n",
      "22, 0.69875\n",
      "\n",
      "23, 0.32577\n",
      "23, 0.70012\n",
      "\n",
      "24, 0.32554\n",
      "24, 0.70204\n",
      "\n",
      "25, 0.32607\n",
      "25, 0.70226\n",
      "\n",
      "26, 0.32251\n",
      "26, 0.71320\n",
      "\n",
      "27, 0.32617\n",
      "27, 0.71314\n",
      "\n",
      "28, 0.32579\n",
      "28, 0.71436\n",
      "\n",
      "29, 0.32617\n",
      "29, 0.71433\n",
      "\n",
      "30, 0.32620\n",
      "30, 0.71416\n",
      "\n",
      "31, 0.32581\n",
      "31, 0.71524\n",
      "\n",
      "32, 0.32532\n",
      "32, 0.71808\n",
      "\n",
      "33, 0.32406\n",
      "33, 0.72377\n",
      "\n",
      "34, 0.32436\n",
      "34, 0.72911\n",
      "\n",
      "35, 0.32522\n",
      "35, 0.73198\n",
      "\n",
      "36, 0.32512\n",
      "36, 0.73489\n",
      "\n",
      "37, 0.32608\n",
      "37, 0.73511\n",
      "\n",
      "38, 0.32619\n",
      "38, 0.73494\n",
      "\n",
      "39, 0.32614\n",
      "39, 0.73495\n",
      "\n",
      "40, 0.32371\n",
      "40, 0.74096\n",
      "\n",
      "41, 0.32619\n",
      "41, 0.74086\n",
      "\n",
      "42, 0.32626\n",
      "42, 0.74062\n",
      "\n",
      "43, 0.32553\n",
      "43, 0.74224\n",
      "\n",
      "44, 0.32344\n",
      "44, 0.74924\n",
      "\n",
      "45, 0.32573\n",
      "45, 0.75055\n",
      "\n",
      "46, 0.32612\n",
      "46, 0.75067\n",
      "\n",
      "47, 0.32622\n",
      "47, 0.75042\n",
      "\n",
      "48, 0.32535\n",
      "48, 0.75248\n",
      "\n",
      "49, 0.32498\n",
      "49, 0.75518\n",
      "\n",
      "50, 0.32603\n",
      "50, 0.75551\n",
      "\n",
      "51, 0.32530\n",
      "51, 0.75749\n",
      "\n",
      "52, 0.32551\n",
      "52, 0.75929\n",
      "\n",
      "53, 0.32319\n",
      "53, 0.76615\n",
      "\n",
      "54, 0.32595\n",
      "54, 0.76662\n",
      "\n",
      "55, 0.32594\n",
      "55, 0.76711\n",
      "\n",
      "56, 0.32497\n",
      "56, 0.76940\n",
      "\n",
      "57, 0.32603\n",
      "57, 0.76968\n",
      "\n",
      "58, 0.32573\n",
      "58, 0.77078\n",
      "\n",
      "59, 0.32610\n",
      "59, 0.77086\n",
      "\n",
      "60, 0.32610\n",
      "60, 0.77106\n",
      "\n",
      "61, 0.32612\n",
      "61, 0.77110\n",
      "\n",
      "62, 0.32603\n",
      "62, 0.77135\n",
      "\n",
      "63, 0.32587\n",
      "63, 0.77208\n",
      "\n",
      "64, 0.32606\n",
      "64, 0.77235\n",
      "\n",
      "65, 0.32611\n",
      "65, 0.77246\n",
      "\n",
      "66, 0.32587\n",
      "66, 0.77317\n",
      "\n",
      "67, 0.32470\n",
      "67, 0.77608\n",
      "\n",
      "68, 0.32604\n",
      "68, 0.77641\n",
      "\n",
      "69, 0.32603\n",
      "69, 0.77673\n",
      "\n",
      "70, 0.32539\n",
      "70, 0.77849\n",
      "\n",
      "71, 0.32560\n",
      "71, 0.77959\n",
      "\n",
      "72, 0.32608\n",
      "72, 0.77968\n",
      "\n",
      "73, 0.32615\n",
      "73, 0.77975\n",
      "\n",
      "74, 0.32585\n",
      "74, 0.78050\n",
      "\n",
      "75, 0.32447\n",
      "75, 0.78355\n",
      "\n",
      "76, 0.32288\n",
      "76, 0.78940\n",
      "\n",
      "77, 0.32594\n",
      "77, 0.78984\n",
      "\n",
      "78, 0.32410\n",
      "78, 0.79369\n",
      "\n",
      "79, 0.32616\n",
      "79, 0.79365\n",
      "\n",
      "80, 0.32272\n",
      "80, 0.79931\n",
      "\n",
      "81, 0.32108\n",
      "81, 0.80777\n",
      "\n",
      "82, 0.32551\n",
      "82, 0.80909\n",
      "\n",
      "83, 0.32540\n",
      "83, 0.81014\n",
      "\n",
      "84, 0.32609\n",
      "84, 0.81027\n",
      "\n",
      "85, 0.32571\n",
      "85, 0.81108\n",
      "\n",
      "86, 0.32419\n",
      "86, 0.81411\n",
      "\n",
      "87, 0.32516\n",
      "87, 0.81545\n",
      "\n",
      "88, 0.32574\n",
      "88, 0.81598\n",
      "\n",
      "89, 0.32497\n",
      "89, 0.81766\n",
      "\n",
      "90, 0.32537\n",
      "106, 0.83625\n",
      "\n",
      "107, 0.32465\n",
      "107, 0.83825\n",
      "\n",
      "108, 0.32413\n",
      "108, 0.84091\n",
      "\n",
      "109, 0.32303\n",
      "109, 0.84448\n",
      "\n",
      "110, 0.32275\n",
      "110, 0.84827\n",
      "\n",
      "111, 0.32561\n",
      "111, 0.84894\n",
      "\n",
      "112, 0.32561\n",
      "112, 0.84964\n",
      "\n",
      "113, 0.32597\n",
      "113, 0.84985\n",
      "\n",
      "114, 0.32564\n",
      "114, 0.85054\n",
      "\n",
      "115, 0.32453\n",
      "115, 0.85230\n",
      "\n",
      "116, 0.32578\n",
      "116, 0.85273\n",
      "\n",
      "117, 0.32627\n",
      "117, 0.85263\n",
      "\n",
      "118, 0.32475\n",
      "118, 0.85429\n",
      "\n",
      "119, 0.32497\n",
      "119, 0.85552\n",
      "\n",
      "120, 0.32601\n",
      "120, 0.85567\n",
      "\n",
      "121, 0.32482\n",
      "121, 0.85742\n",
      "\n",
      "122, 0.32462\n",
      "122, 0.85926\n",
      "\n",
      "123, 0.32418\n",
      "123, 0.86119\n",
      "\n",
      "124, 0.32617\n",
      "124, 0.86117\n",
      "\n",
      "125, 0.32559\n",
      "125, 0.86175\n",
      "\n",
      "126, 0.32617\n",
      "126, 0.86175\n",
      "\n",
      "127, 0.32476\n",
      "127, 0.86341\n",
      "\n",
      "128, 0.32561\n",
      "128, 0.86409\n",
      "\n",
      "129, 0.32619\n",
      "129, 0.86404\n",
      "\n",
      "130, 0.32542\n",
      "130, 0.86508\n",
      "\n",
      "131, 0.32555\n",
      "131, 0.86572\n",
      "\n",
      "132, 0.32559\n",
      "132, 0.86630\n",
      "\n",
      "133, 0.32414\n",
      "133, 0.86845\n",
      "\n",
      "134, 0.32571\n",
      "134, 0.86873\n",
      "\n",
      "135, 0.32513\n",
      "135, 0.86977\n",
      "\n",
      "136, 0.32616\n",
      "136, 0.86976\n",
      "\n",
      "137, 0.32549\n",
      "137, 0.87048\n",
      "\n",
      "138, 0.32592\n",
      "138, 0.87060\n",
      "\n",
      "139, 0.32151\n",
      "139, 0.87491\n",
      "\n",
      "140, 0.32606\n",
      "140, 0.87500\n",
      "\n",
      "141, 0.32505\n",
      "141, 0.87600\n",
      "\n",
      "142, 0.32582\n",
      "142, 0.87630\n",
      "\n",
      "143, 0.32608\n",
      "143, 0.87637\n",
      "\n",
      "144, 0.32594\n",
      "144, 0.87664\n",
      "\n",
      "145, 0.32519\n",
      "145, 0.87743\n",
      "\n",
      "146, 0.32302\n",
      "146, 0.88012\n",
      "\n",
      "147, 0.32414\n",
      "147, 0.88187\n",
      "\n",
      "148, 0.32411\n",
      "148, 0.88353\n",
      "\n",
      "149, 0.32462\n",
      "149, 0.88487\n",
      "\n",
      "150, 0.32553\n",
      "150, 0.88547\n",
      "\n",
      "151, 0.32545\n",
      "151, 0.88598\n",
      "\n",
      "152, 0.32602\n",
      "152, 0.88609\n",
      "\n",
      "153, 0.32613\n",
      "153, 0.88611\n",
      "\n",
      "154, 0.32411\n",
      "154, 0.88749\n",
      "\n",
      "155, 0.32471\n",
      "155, 0.88864\n",
      "\n",
      "156, 0.32588\n",
      "156, 0.88889\n",
      "\n",
      "157, 0.32509\n",
      "157, 0.88980\n",
      "\n",
      "158, 0.32617\n",
      "158, 0.88980\n",
      "\n",
      "159, 0.32607\n",
      "159, 0.88986\n",
      "\n",
      "160, 0.32610\n",
      "160, 0.88993\n",
      "\n",
      "161, 0.32616\n",
      "161, 0.88993\n",
      "\n",
      "162, 0.32534\n",
      "162, 0.89051\n",
      "\n",
      "163, 0.32492\n",
      "163, 0.89146\n",
      "\n",
      "164, 0.32391\n",
      "164, 0.89301\n",
      "\n",
      "165, 0.32363\n",
      "165, 0.89504\n",
      "\n",
      "166, 0.32376\n",
      "166, 0.89673\n",
      "\n",
      "167, 0.32525\n",
      "167, 0.89740\n",
      "\n",
      "168, 0.32596\n",
      "168, 0.89763\n",
      "\n",
      "169, 0.32465\n",
      "169, 0.89882\n",
      "\n",
      "170, 0.32392\n",
      "170, 0.90028\n",
      "\n",
      "171, 0.32594\n",
      "171, 0.90042\n",
      "\n",
      "172, 0.32476\n",
      "172, 0.90151\n",
      "\n",
      "173, 0.32471\n",
      "173, 0.90249\n",
      "\n",
      "174, 0.32254\n",
      "174, 0.90449\n",
      "\n",
      "175, 0.32575\n",
      "175, 0.90484\n",
      "\n",
      "176, 0.32613\n",
      "176, 0.90484\n",
      "\n",
      "177, 0.32450\n",
      "177, 0.90590\n",
      "\n",
      "178, 0.32586\n",
      "178, 0.90607\n",
      "\n",
      "179, 0.32368\n",
      "179, 0.90730\n",
      "\n",
      "180, 0.32513\n",
      "180, 0.90801\n",
      "\n",
      "181, 0.32604\n",
      "181, 0.90810\n",
      "\n",
      "182, 0.32630\n",
      "182, 0.90801\n",
      "\n",
      "183, 0.32616\n",
      "183, 0.90800\n",
      "\n",
      "184, 0.32392\n",
      "184, 0.90949\n",
      "\n",
      "185, 0.32618\n",
      "185, 0.90947\n",
      "\n",
      "186, 0.32554\n",
      "186, 0.90978\n",
      "\n",
      "187, 0.32573\n",
      "187, 0.91015\n",
      "\n",
      "188, 0.32488\n",
      "188, 0.91099\n",
      "\n",
      "189, 0.32612\n",
      "189, 0.91099\n",
      "\n",
      "190, 0.32369\n",
      "190, 0.91246\n",
      "\n",
      "191, 0.32437\n",
      "191, 0.91351\n",
      "\n",
      "192, 0.32507\n",
      "192, 0.91419\n",
      "\n",
      "193, 0.32593\n",
      "193, 0.91438\n",
      "\n",
      "194, 0.32579\n",
      "194, 0.91457\n",
      "\n",
      "195, 0.32564\n",
      "195, 0.91489\n",
      "\n",
      "196, 0.32560\n",
      "196, 0.91515\n",
      "\n",
      "197, 0.32525\n",
      "197, 0.91565\n",
      "\n",
      "198, 0.32357\n",
      "198, 0.91723\n",
      "\n",
      "199, 0.32562\n",
      "199, 0.91756\n",
      "\n",
      "0.9176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=3, random_state=1)\n",
    "def check_score(results):\n",
    "    predictions = []\n",
    "    for i in tqdm.tqdm_notebook(range(200)):\n",
    "        f = 'var_%d' % i\n",
    "        f_vc = '%s_vc' % f\n",
    "        runs = sorted(results[i], key=lambda x: x[1])\n",
    "        \n",
    "        p_folds = []\n",
    "        for j in range(5): \n",
    "            _, _, parameters, _ = runs[j]\n",
    "            fixed_parameters = dict(parameters)\n",
    "            fixed_parameters['n_estimators'] = fixed_parameters['n_estimators']\n",
    "\n",
    "            model = LGBMClassifier(**fixed_parameters)\n",
    "\n",
    "            p = cross_val_predict(model, train[[f, f_vc]].values, train['target'], cv=cv, method='predict_proba')[:, 1]\n",
    "            #print '%d, %d, %.5f' % (i, j, log_loss(train['target'], p))\n",
    "            p_folds.append(p)\n",
    "            \n",
    "        p = np.mean(np.column_stack(p_folds), axis=1)\n",
    "        print '%d, %.5f' % (i, log_loss(train['target'], p))\n",
    "        predictions.append(p)\n",
    "        print '%d, %.5f' % (i, roc_auc_score(train['target'], aggregate(np.column_stack(predictions))))\n",
    "        print\n",
    "        \n",
    "    predictions = np.column_stack(predictions)    \n",
    "    return predictions\n",
    "\n",
    "p_train = check_score(results)\n",
    "print '%.4f' % (roc_auc_score(train['target'], aggregate(p_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78290e742cf14786ac60b218711de334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.32388\n",
      "0, 0.54657\n",
      "\n",
      "1, 0.32396\n",
      "1, 0.57296\n",
      "\n",
      "2, 0.32357\n",
      "2, 0.59543\n",
      "\n",
      "3, 0.32605\n",
      "3, 0.59653\n",
      "\n",
      "4, 0.32606\n",
      "4, 0.59739\n",
      "\n",
      "5, 0.32457\n",
      "5, 0.60640\n",
      "\n",
      "6, 0.32325\n",
      "6, 0.62706\n",
      "\n",
      "7, 0.32618\n",
      "7, 0.62690\n",
      "\n",
      "8, 0.32583\n",
      "8, 0.62900\n",
      "\n",
      "9, 0.32449\n",
      "9, 0.63931\n",
      "\n",
      "10, 0.32614\n",
      "10, 0.63948\n",
      "\n",
      "11, 0.32570\n",
      "11, 0.64195\n",
      "\n",
      "12, 0.32265\n",
      "12, 0.65894\n",
      "\n",
      "13, 0.32353\n",
      "13, 0.66985\n",
      "\n",
      "14, 0.32614\n",
      "14, 0.66998\n",
      "\n",
      "15, 0.32596\n",
      "15, 0.67091\n",
      "\n",
      "16, 0.32617\n",
      "16, 0.67062\n",
      "\n",
      "17, 0.32617\n",
      "17, 0.67059\n",
      "\n",
      "18, 0.32427\n",
      "18, 0.67812\n",
      "\n",
      "19, 0.32589\n",
      "19, 0.67916\n",
      "\n",
      "20, 0.32577\n",
      "20, 0.68092\n",
      "\n",
      "21, 0.32339\n",
      "21, 0.69133\n",
      "\n",
      "22, 0.32330\n",
      "22, 0.70079\n",
      "\n",
      "23, 0.32577\n",
      "23, 0.70219\n",
      "\n",
      "24, 0.32550\n",
      "24, 0.70430\n",
      "\n",
      "25, 0.32607\n",
      "25, 0.70452\n",
      "\n",
      "26, 0.32250\n",
      "26, 0.71523\n",
      "\n",
      "27, 0.32618\n",
      "27, 0.71516\n",
      "\n",
      "28, 0.32579\n",
      "28, 0.71632\n",
      "\n",
      "29, 0.32617\n",
      "29, 0.71631\n",
      "\n",
      "30, 0.32619\n",
      "30, 0.71617\n",
      "\n",
      "31, 0.32579\n",
      "31, 0.71729\n",
      "\n",
      "32, 0.32531\n",
      "32, 0.72014\n",
      "\n",
      "33, 0.32405\n",
      "33, 0.72582\n",
      "\n",
      "34, 0.32435\n",
      "34, 0.73106\n",
      "\n",
      "35, 0.32523\n",
      "35, 0.73389\n",
      "\n",
      "36, 0.32508\n",
      "36, 0.73695\n",
      "\n",
      "37, 0.32608\n",
      "37, 0.73719\n",
      "\n",
      "38, 0.32619\n",
      "38, 0.73702\n",
      "\n",
      "39, 0.32614\n",
      "39, 0.73705\n",
      "\n",
      "40, 0.32366\n",
      "40, 0.74325\n",
      "\n",
      "41, 0.32617\n",
      "41, 0.74320\n",
      "\n",
      "42, 0.32623\n",
      "42, 0.74299\n",
      "\n",
      "43, 0.32555\n",
      "43, 0.74450\n",
      "\n",
      "44, 0.32339\n",
      "44, 0.75160\n",
      "\n",
      "45, 0.32569\n",
      "45, 0.75287\n",
      "\n",
      "46, 0.32612\n",
      "46, 0.75289\n",
      "\n",
      "47, 0.32620\n",
      "47, 0.75282\n",
      "\n",
      "48, 0.32530\n",
      "48, 0.75498\n",
      "\n",
      "49, 0.32498\n",
      "49, 0.75764\n",
      "\n",
      "50, 0.32603\n",
      "50, 0.75792\n",
      "\n",
      "51, 0.32523\n",
      "51, 0.75996\n",
      "\n",
      "52, 0.32552\n",
      "52, 0.76168\n",
      "\n",
      "53, 0.32313\n",
      "53, 0.76851\n",
      "\n",
      "54, 0.32594\n",
      "54, 0.76898\n",
      "\n",
      "55, 0.32585\n",
      "55, 0.76966\n",
      "\n",
      "56, 0.32490\n",
      "56, 0.77196\n",
      "\n",
      "57, 0.32603\n",
      "57, 0.77226\n",
      "\n",
      "58, 0.32570\n",
      "58, 0.77340\n",
      "\n",
      "59, 0.32611\n",
      "59, 0.77347\n",
      "\n",
      "60, 0.32608\n",
      "60, 0.77369\n",
      "\n",
      "61, 0.32607\n",
      "61, 0.77380\n",
      "\n",
      "62, 0.32603\n",
      "62, 0.77402\n",
      "\n",
      "63, 0.32586\n",
      "63, 0.77471\n",
      "\n",
      "64, 0.32604\n",
      "64, 0.77507\n",
      "\n",
      "65, 0.32611\n",
      "65, 0.77518\n",
      "\n",
      "66, 0.32587\n",
      "66, 0.77586\n",
      "\n",
      "67, 0.32467\n",
      "67, 0.77879\n",
      "\n",
      "68, 0.32604\n",
      "68, 0.77911\n",
      "\n",
      "69, 0.32606\n",
      "69, 0.77938\n",
      "\n",
      "70, 0.32538\n",
      "70, 0.78115\n",
      "\n",
      "71, 0.32556\n",
      "71, 0.78233\n",
      "\n",
      "72, 0.32607\n",
      "72, 0.78242\n",
      "\n",
      "73, 0.32612\n",
      "73, 0.78251\n",
      "\n",
      "74, 0.32582\n",
      "74, 0.78327\n",
      "\n",
      "75, 0.32446\n",
      "75, 0.78632\n",
      "\n",
      "76, 0.32287\n",
      "76, 0.79204\n",
      "\n",
      "77, 0.32591\n",
      "77, 0.79253\n",
      "\n",
      "78, 0.32401\n",
      "78, 0.79654\n",
      "\n",
      "79, 0.32613\n",
      "79, 0.79656\n",
      "\n",
      "80, 0.32272\n",
      "80, 0.80220\n",
      "\n",
      "81, 0.32102\n",
      "81, 0.81068\n",
      "\n",
      "82, 0.32541\n",
      "82, 0.81219\n",
      "\n",
      "83, 0.32538\n",
      "83, 0.81326\n",
      "\n",
      "84, 0.32608\n",
      "84, 0.81339\n",
      "\n",
      "85, 0.32571\n",
      "85, 0.81414\n",
      "\n",
      "86, 0.32418\n",
      "86, 0.81718\n",
      "\n",
      "87, 0.32514\n",
      "87, 0.81859\n",
      "\n",
      "88, 0.32571\n",
      "88, 0.81913\n",
      "\n",
      "89, 0.32497\n",
      "89, 0.82081\n",
      "\n",
      "90, 0.32534\n",
      "90, 0.82205\n",
      "\n",
      "91, 0.32500\n",
      "91, 0.82381\n",
      "\n",
      "92, 0.32405\n",
      "92, 0.82702\n",
      "\n",
      "93, 0.32529\n",
      "93, 0.82813\n",
      "\n",
      "94, 0.32412\n",
      "94, 0.83074\n",
      "\n",
      "95, 0.32507\n",
      "95, 0.83219\n",
      "\n",
      "96, 0.32617\n",
      "96, 0.83217\n",
      "\n",
      "97, 0.32580\n",
      "97, 0.83283\n",
      "\n",
      "98, 0.32615\n",
      "98, 0.83282\n",
      "\n",
      "99, 0.32358\n",
      "99, 0.83638\n",
      "\n",
      "100, 0.32617\n",
      "100, 0.83634\n",
      "\n",
      "101, 0.32609\n",
      "101, 0.83628\n",
      "\n",
      "102, 0.32556\n",
      "102, 0.83706\n",
      "\n",
      "103, 0.32615\n",
      "103, 0.83706\n",
      "\n",
      "104, 0.32577\n",
      "104, 0.83756\n",
      "\n",
      "105, 0.32579\n",
      "105, 0.83808\n",
      "\n",
      "106, 0.32540\n",
      "106, 0.83909\n",
      "\n",
      "107, 0.32465\n",
      "107, 0.84106\n",
      "\n",
      "108, 0.32415\n",
      "108, 0.84367\n",
      "\n",
      "109, 0.32305\n",
      "109, 0.84714\n",
      "\n",
      "110, 0.32269\n",
      "110, 0.85089\n",
      "\n",
      "111, 0.32560\n",
      "111, 0.85152\n",
      "\n",
      "112, 0.32558\n",
      "112, 0.85225\n",
      "\n",
      "113, 0.32592\n",
      "113, 0.85256\n",
      "\n",
      "114, 0.32561\n",
      "114, 0.85328\n",
      "\n",
      "115, 0.32454\n",
      "115, 0.85494\n",
      "\n",
      "116, 0.32576\n",
      "116, 0.85536\n",
      "\n",
      "117, 0.32620\n",
      "117, 0.85533\n",
      "\n",
      "118, 0.32473\n",
      "118, 0.85690\n",
      "\n",
      "119, 0.32496\n",
      "119, 0.85816\n",
      "\n",
      "120, 0.32600\n",
      "120, 0.85833\n",
      "\n",
      "121, 0.32481\n",
      "121, 0.86004\n",
      "\n",
      "122, 0.32459\n",
      "122, 0.86189\n",
      "\n",
      "123, 0.32418\n",
      "123, 0.86373\n",
      "\n",
      "124, 0.32616\n",
      "124, 0.86372\n",
      "\n",
      "125, 0.32558\n",
      "125, 0.86429\n",
      "\n",
      "126, 0.32617\n",
      "126, 0.86426\n",
      "\n",
      "127, 0.32479\n",
      "127, 0.86583\n",
      "\n",
      "128, 0.32557\n",
      "128, 0.86654\n",
      "\n",
      "129, 0.32614\n",
      "129, 0.86655\n",
      "\n",
      "130, 0.32536\n",
      "130, 0.86756\n",
      "\n",
      "131, 0.32548\n",
      "131, 0.86827\n",
      "\n",
      "132, 0.32558\n",
      "132, 0.86887\n",
      "\n",
      "133, 0.32412\n",
      "133, 0.87098\n",
      "\n",
      "134, 0.32570\n",
      "134, 0.87125\n",
      "\n",
      "135, 0.32511\n",
      "135, 0.87218\n",
      "\n",
      "136, 0.32616\n",
      "136, 0.87218\n",
      "\n",
      "137, 0.32547\n",
      "137, 0.87290\n",
      "\n",
      "138, 0.32591\n",
      "138, 0.87311\n",
      "\n",
      "139, 0.32140\n",
      "139, 0.87741\n",
      "\n",
      "140, 0.32605\n",
      "140, 0.87750\n",
      "\n",
      "141, 0.32502\n",
      "141, 0.87856\n",
      "\n",
      "142, 0.32583\n",
      "142, 0.87883\n",
      "\n",
      "143, 0.32605\n",
      "143, 0.87892\n",
      "\n",
      "144, 0.32588\n",
      "144, 0.87923\n",
      "\n",
      "145, 0.32515\n",
      "145, 0.88004\n",
      "\n",
      "146, 0.32301\n",
      "146, 0.88271\n",
      "\n",
      "147, 0.32409\n",
      "147, 0.88443\n",
      "\n",
      "148, 0.32406\n",
      "148, 0.88605\n",
      "\n",
      "149, 0.32458\n",
      "149, 0.88741\n",
      "\n",
      "150, 0.32552\n",
      "150, 0.88801\n",
      "\n",
      "151, 0.32542\n",
      "151, 0.88850\n",
      "\n",
      "152, 0.32601\n",
      "152, 0.88860\n",
      "\n",
      "153, 0.32613\n",
      "153, 0.88862\n",
      "\n",
      "154, 0.32408\n",
      "154, 0.88994\n",
      "\n",
      "155, 0.32466\n",
      "155, 0.89113\n",
      "\n",
      "156, 0.32586\n",
      "156, 0.89138\n",
      "\n",
      "157, 0.32506\n",
      "157, 0.89226\n",
      "\n",
      "158, 0.32616\n",
      "158, 0.89227\n",
      "\n",
      "159, 0.32606\n",
      "159, 0.89234\n",
      "\n",
      "160, 0.32612\n",
      "160, 0.89238\n",
      "\n",
      "161, 0.32615\n",
      "161, 0.89238\n",
      "\n",
      "162, 0.32530\n",
      "162, 0.89298\n",
      "\n",
      "163, 0.32491\n",
      "163, 0.89394\n",
      "\n",
      "164, 0.32386\n",
      "164, 0.89546\n",
      "\n",
      "165, 0.32357\n",
      "165, 0.89749\n",
      "\n",
      "166, 0.32363\n",
      "166, 0.89931\n",
      "\n",
      "167, 0.32523\n",
      "167, 0.89997\n",
      "\n",
      "168, 0.32592\n",
      "168, 0.90021\n",
      "\n",
      "169, 0.32462\n",
      "169, 0.90139\n",
      "\n",
      "170, 0.32387\n",
      "170, 0.90286\n",
      "\n",
      "171, 0.32593\n",
      "171, 0.90298\n",
      "\n",
      "172, 0.32475\n",
      "172, 0.90403\n",
      "\n",
      "173, 0.32469\n",
      "173, 0.90500\n",
      "\n",
      "174, 0.32257\n",
      "174, 0.90696\n",
      "\n",
      "175, 0.32572\n",
      "175, 0.90730\n",
      "\n",
      "176, 0.32612\n",
      "176, 0.90732\n",
      "\n",
      "177, 0.32448\n",
      "177, 0.90836\n",
      "\n",
      "178, 0.32584\n",
      "178, 0.90856\n",
      "\n",
      "179, 0.32361\n",
      "179, 0.90978\n",
      "\n",
      "180, 0.32511\n",
      "180, 0.91048\n",
      "\n",
      "181, 0.32603\n",
      "181, 0.91056\n",
      "\n",
      "182, 0.32612\n",
      "182, 0.91060\n",
      "\n",
      "183, 0.32616\n",
      "183, 0.91061\n",
      "\n",
      "184, 0.32389\n",
      "184, 0.91205\n",
      "\n",
      "185, 0.32618\n",
      "185, 0.91203\n",
      "\n",
      "186, 0.32552\n",
      "186, 0.91235\n",
      "\n",
      "187, 0.32569\n",
      "187, 0.91274\n",
      "\n",
      "188, 0.32487\n",
      "188, 0.91361\n",
      "\n",
      "189, 0.32612\n",
      "189, 0.91359\n",
      "\n",
      "190, 0.32361\n",
      "190, 0.91502\n",
      "\n",
      "191, 0.32429\n",
      "191, 0.91604\n",
      "\n",
      "192, 0.32508\n",
      "192, 0.91671\n",
      "\n",
      "193, 0.32591\n",
      "193, 0.91689\n",
      "\n",
      "194, 0.32576\n",
      "194, 0.91709\n",
      "\n",
      "195, 0.32559\n",
      "195, 0.91744\n",
      "\n",
      "196, 0.32554\n",
      "196, 0.91769\n",
      "\n",
      "197, 0.32522\n",
      "197, 0.91823\n",
      "\n",
      "198, 0.32351\n",
      "198, 0.91978\n",
      "\n",
      "199, 0.32561\n",
      "199, 0.92009\n",
      "\n",
      "0.9201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=10, random_state=1)\n",
    "\n",
    "p_train = check_score(results)\n",
    "print '%.4f' % (roc_auc_score(train['target'], aggregate(p_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9957f0b3620644cdb308120956b8499d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0, 0.54688, 0.32340, 0.32390\n",
      "0, 1, 0.54852, 0.32342, 0.32396\n",
      "0, 2, 0.54876, 0.32343, 0.32391\n",
      "0, 3, 0.54725, 0.32344, 0.32390\n",
      "0, 4, 0.54615, 0.32346, 0.32389\n",
      "0, 0.32388\n",
      "0, 0.54657\n",
      "\n",
      "1, 0, 0.54009, 0.32418, 0.32398\n",
      "1, 1, 0.54133, 0.32419, 0.32402\n",
      "1, 2, 0.54042, 0.32419, 0.32397\n",
      "1, 3, 0.53986, 0.32420, 0.32400\n",
      "1, 4, 0.53892, 0.32420, 0.32398\n",
      "1, 0.32396\n",
      "1, 0.57296\n",
      "\n",
      "2, 0, 0.55449, 0.32276, 0.32358\n",
      "2, 1, 0.55573, 0.32276, 0.32356\n",
      "2, 2, 0.55560, 0.32276, 0.32355\n",
      "2, 3, 0.55458, 0.32278, 0.32364\n",
      "2, 4, 0.55635, 0.32278, 0.32357\n",
      "2, 0.32357\n",
      "2, 0.59543\n",
      "\n",
      "3, 0, 0.50529, 0.32575, 0.32604\n",
      "3, 1, 0.50560, 0.32576, 0.32604\n",
      "3, 2, 0.50739, 0.32576, 0.32609\n",
      "3, 3, 0.50615, 0.32576, 0.32603\n",
      "3, 4, 0.50643, 0.32577, 0.32611\n",
      "3, 0.32605\n",
      "3, 0.59653\n",
      "\n",
      "4, 0, 0.50182, 0.32578, 0.32608\n",
      "4, 1, 0.50721, 0.32580, 0.32606\n",
      "4, 2, 0.50630, 0.32580, 0.32606\n",
      "4, 3, 0.50397, 0.32580, 0.32609\n",
      "4, 4, 0.50243, 0.32581, 0.32606\n",
      "4, 0.32606\n",
      "4, 0.59739\n",
      "\n",
      "5, 0, 0.53352, 0.32437, 0.32458\n",
      "5, 1, 0.53370, 0.32441, 0.32462\n",
      "5, 2, 0.53457, 0.32443, 0.32458\n",
      "5, 3, 0.53206, 0.32445, 0.32458\n",
      "5, 4, 0.52972, 0.32447, 0.32461\n",
      "5, 0.32457\n",
      "5, 0.60640\n",
      "\n",
      "6, 0, 0.56279, 0.32287, 0.32324\n",
      "6, 1, 0.56446, 0.32287, 0.32324\n",
      "6, 2, 0.56205, 0.32287, 0.32326\n",
      "6, 3, 0.56211, 0.32288, 0.32326\n",
      "6, 4, 0.56231, 0.32289, 0.32328\n",
      "6, 0.32325\n",
      "6, 0.62706\n",
      "\n",
      "7, 0, 0.50411, 0.32582, 0.32619\n",
      "7, 1, 0.50108, 0.32584, 0.32616\n",
      "7, 2, 0.50775, 0.32585, 0.32623\n",
      "7, 3, 0.49979, 0.32586, 0.32617\n",
      "7, 4, 0.50014, 0.32588, 0.32618\n",
      "7, 0.32618\n",
      "7, 0.62690\n",
      "\n",
      "8, 0, 0.51928, 0.32552, 0.32582\n",
      "8, 1, 0.51966, 0.32553, 0.32582\n",
      "8, 2, 0.51630, 0.32554, 0.32582\n",
      "8, 3, 0.51937, 0.32557, 0.32588\n",
      "8, 4, 0.51832, 0.32557, 0.32588\n",
      "8, 0.32583\n",
      "8, 0.62900\n",
      "\n",
      "9, 0, 0.54491, 0.32394, 0.32453\n",
      "9, 1, 0.54479, 0.32400, 0.32450\n",
      "9, 2, 0.54439, 0.32400, 0.32452\n",
      "9, 3, 0.54546, 0.32402, 0.32452\n",
      "9, 4, 0.54667, 0.32402, 0.32447\n",
      "9, 0.32449\n",
      "9, 0.63931\n",
      "\n",
      "10, 0, 0.50703, 0.32584, 0.32617\n",
      "10, 1, 0.49976, 0.32584, 0.32614\n",
      "10, 2, 0.49635, 0.32585, 0.32614\n",
      "10, 3, 0.50514, 0.32586, 0.32615\n",
      "10, 4, 0.49715, 0.32586, 0.32614\n",
      "10, 0.32614\n",
      "10, 0.63948\n",
      "\n",
      "11, 0, 0.52087, 0.32529, 0.32570\n",
      "11, 1, 0.52384, 0.32529, 0.32572\n",
      "11, 2, 0.52023, 0.32530, 0.32571\n",
      "11, 3, 0.52162, 0.32530, 0.32570\n",
      "11, 4, 0.52290, 0.32532, 0.32571\n",
      "11, 0.32570\n",
      "11, 0.64195\n",
      "\n",
      "12, 0, 0.56262, 0.32191, 0.32272\n",
      "12, 1, 0.56110, 0.32192, 0.32264\n",
      "12, 2, 0.55928, 0.32192, 0.32264\n",
      "12, 3, 0.56061, 0.32193, 0.32264\n",
      "12, 4, 0.56198, 0.32194, 0.32265\n",
      "12, 0.32265\n",
      "12, 0.65894\n",
      "\n",
      "13, 0, 0.55140, 0.32336, 0.32351\n",
      "13, 1, 0.55110, 0.32339, 0.32359\n",
      "13, 2, 0.55080, 0.32340, 0.32351\n",
      "13, 3, 0.55125, 0.32340, 0.32356\n",
      "13, 4, 0.55203, 0.32340, 0.32357\n",
      "13, 0.32353\n",
      "13, 0.66985\n",
      "\n",
      "14, 0, 0.50288, 0.32585, 0.32615\n",
      "14, 1, 0.50185, 0.32585, 0.32615\n",
      "14, 2, 0.50330, 0.32585, 0.32615\n",
      "14, 3, 0.50119, 0.32586, 0.32615\n",
      "14, 4, 0.50569, 0.32587, 0.32614\n",
      "14, 0.32614\n",
      "14, 0.66998\n",
      "\n",
      "15, 0, 0.51998, 0.32551, 0.32597\n",
      "15, 1, 0.51974, 0.32552, 0.32596\n",
      "15, 2, 0.52106, 0.32552, 0.32597\n",
      "15, 3, 0.52299, 0.32552, 0.32596\n",
      "15, 4, 0.52072, 0.32554, 0.32596\n",
      "15, 0.32596\n",
      "15, 0.67091\n",
      "\n",
      "16, 0, 0.51761, 0.32563, 0.32626\n",
      "16, 1, 0.51471, 0.32563, 0.32617\n",
      "16, 2, 0.51723, 0.32565, 0.32619\n",
      "16, 3, 0.51668, 0.32569, 0.32618\n",
      "16, 4, 0.51304, 0.32570, 0.32621\n",
      "16, 0.32617\n",
      "16, 0.67062\n",
      "\n",
      "17, 0, 0.50535, 0.32583, 0.32618\n",
      "17, 1, 0.50172, 0.32585, 0.32618\n",
      "17, 2, 0.50719, 0.32585, 0.32618\n",
      "17, 3, 0.50646, 0.32585, 0.32617\n",
      "17, 4, 0.50548, 0.32586, 0.32617\n",
      "17, 0.32617\n",
      "17, 0.67059\n",
      "\n",
      "18, 0, 0.53713, 0.32364, 0.32434\n",
      "18, 1, 0.53589, 0.32367, 0.32426\n",
      "18, 2, 0.53607, 0.32367, 0.32424\n",
      "18, 3, 0.53649, 0.32368, 0.32432\n",
      "18, 4, 0.53537, 0.32369, 0.32429\n",
      "18, 0.32427\n",
      "18, 0.67812\n",
      "\n",
      "19, 0, 0.51781, 0.32539, 0.32596\n",
      "19, 1, 0.52192, 0.32539, 0.32592\n",
      "19, 2, 0.51484, 0.32541, 0.32590\n",
      "19, 3, 0.51836, 0.32542, 0.32590\n",
      "19, 4, 0.51551, 0.32542, 0.32590\n",
      "19, 0.32589\n",
      "19, 0.67916\n",
      "\n",
      "20, 0, 0.52502, 0.32523, 0.32581\n",
      "20, 1, 0.52406, 0.32526, 0.32593\n",
      "20, 2, 0.52503, 0.32531, 0.32577\n",
      "20, 3, 0.52203, 0.32531, 0.32575\n",
      "20, 4, 0.52187, 0.32532, 0.32579\n",
      "20, 0.32577\n",
      "20, 0.68092\n",
      "\n",
      "21, 0, 0.55586, 0.32305, 0.32344\n",
      "21, 1, 0.55587, 0.32306, 0.32338\n",
      "21, 2, 0.55546, 0.32307, 0.32337\n",
      "21, 3, 0.55460, 0.32310, 0.32351\n",
      "21, 4, 0.55487, 0.32310, 0.32339\n",
      "21, 0.32339\n",
      "21, 0.69133\n",
      "\n",
      "22, 0, 0.55063, 0.32294, 0.32332\n",
      "22, 1, 0.55032, 0.32298, 0.32331\n",
      "22, 2, 0.55070, 0.32299, 0.32333\n",
      "22, 3, 0.54724, 0.32304, 0.32332\n",
      "22, 4, 0.54953, 0.32304, 0.32333\n",
      "22, 0.32330\n",
      "22, 0.70079\n",
      "\n",
      "23, 0, 0.52514, 0.32549, 0.32583\n",
      "23, 1, 0.52240, 0.32552, 0.32576\n",
      "23, 2, 0.52449, 0.32553, 0.32577\n",
      "23, 3, 0.52542, 0.32554, 0.32577\n",
      "23, 4, 0.52294, 0.32554, 0.32577\n",
      "23, 0.32577\n",
      "23, 0.70219\n",
      "\n",
      "24, 0, 0.53785, 0.32473, 0.32562\n",
      "24, 1, 0.53960, 0.32476, 0.32551\n",
      "24, 2, 0.53766, 0.32477, 0.32548\n",
      "24, 3, 0.53572, 0.32478, 0.32553\n",
      "24, 4, 0.53864, 0.32478, 0.32554\n",
      "24, 0.32550\n",
      "24, 0.70430\n",
      "\n",
      "25, 0, 0.50610, 0.32581, 0.32609\n",
      "25, 1, 0.50218, 0.32583, 0.32610\n",
      "25, 2, 0.50194, 0.32584, 0.32609\n",
      "25, 3, 0.50225, 0.32586, 0.32606\n",
      "25, 4, 0.50216, 0.32589, 0.32607\n",
      "25, 0.32607\n",
      "25, 0.70452\n",
      "\n",
      "26, 0, 0.55251, 0.32234, 0.32254\n",
      "26, 1, 0.55255, 0.32235, 0.32249\n",
      "26, 2, 0.55313, 0.32235, 0.32248\n",
      "26, 3, 0.55350, 0.32237, 0.32260\n",
      "26, 4, 0.55112, 0.32242, 0.32251\n",
      "26, 0.32250\n",
      "26, 0.71523\n",
      "\n",
      "27, 0, 0.50444, 0.32583, 0.32620\n",
      "27, 1, 0.50816, 0.32584, 0.32618\n",
      "27, 2, 0.50912, 0.32584, 0.32617\n",
      "27, 3, 0.50714, 0.32584, 0.32618\n",
      "27, 4, 0.50325, 0.32586, 0.32619\n",
      "27, 0.32618\n",
      "27, 0.71516\n",
      "\n",
      "28, 0, 0.52446, 0.32552, 0.32587\n",
      "28, 1, 0.52299, 0.32553, 0.32578\n",
      "28, 2, 0.52182, 0.32554, 0.32578\n",
      "28, 3, 0.52259, 0.32555, 0.32578\n",
      "28, 4, 0.52315, 0.32557, 0.32588\n",
      "28, 0.32579\n",
      "28, 0.71632\n",
      "\n",
      "29, 0, 0.49952, 0.32585, 0.32615\n",
      "29, 1, 0.50161, 0.32587, 0.32616\n",
      "29, 2, 0.50739, 0.32587, 0.32617\n",
      "29, 3, 0.49820, 0.32589, 0.32618\n",
      "29, 4, 0.50107, 0.32589, 0.32617\n",
      "29, 0.32617\n",
      "29, 0.71631\n",
      "\n",
      "30, 0, 0.50709, 0.32583, 0.32624\n",
      "30, 1, 0.50758, 0.32584, 0.32619\n",
      "30, 2, 0.50300, 0.32585, 0.32616\n",
      "30, 3, 0.50441, 0.32586, 0.32621\n",
      "30, 4, 0.50170, 0.32586, 0.32618\n",
      "30, 0.32619\n",
      "30, 0.71617\n",
      "\n",
      "31, 0, 0.52526, 0.32552, 0.32580\n",
      "31, 1, 0.52423, 0.32552, 0.32581\n",
      "31, 2, 0.52370, 0.32552, 0.32578\n",
      "31, 3, 0.52534, 0.32552, 0.32579\n",
      "31, 4, 0.52288, 0.32555, 0.32579\n",
      "31, 0.32579\n",
      "31, 0.71729\n",
      "\n",
      "32, 0, 0.51965, 0.32535, 0.32543\n",
      "32, 1, 0.51812, 0.32542, 0.32525\n",
      "32, 2, 0.51974, 0.32544, 0.32548\n",
      "32, 3, 0.51989, 0.32546, 0.32525\n",
      "32, 4, 0.51912, 0.32548, 0.32542\n",
      "32, 0.32531\n",
      "32, 0.72014\n",
      "\n",
      "33, 0, 0.54026, 0.32343, 0.32405\n",
      "33, 1, 0.53921, 0.32343, 0.32407\n",
      "33, 2, 0.53960, 0.32343, 0.32407\n",
      "33, 3, 0.54160, 0.32343, 0.32406\n",
      "33, 4, 0.53929, 0.32344, 0.32407\n",
      "33, 0.32405\n",
      "33, 0.72582\n",
      "\n",
      "34, 0, 0.54775, 0.32417, 0.32436\n",
      "34, 1, 0.54689, 0.32417, 0.32437\n",
      "34, 2, 0.54951, 0.32417, 0.32435\n",
      "34, 3, 0.54871, 0.32418, 0.32438\n",
      "34, 4, 0.54764, 0.32418, 0.32437\n",
      "34, 0.32435\n",
      "34, 0.73106\n",
      "\n",
      "35, 0, 0.53124, 0.32529, 0.32533\n",
      "35, 1, 0.53001, 0.32532, 0.32517\n",
      "35, 2, 0.53227, 0.32533, 0.32535\n",
      "35, 3, 0.53204, 0.32534, 0.32516\n",
      "35, 4, 0.53071, 0.32537, 0.32529\n",
      "35, 0.32523\n",
      "35, 0.73389\n",
      "\n",
      "36, 0, 0.54390, 0.32448, 0.32507\n",
      "36, 1, 0.54323, 0.32449, 0.32508\n",
      "36, 2, 0.54302, 0.32449, 0.32509\n",
      "36, 3, 0.54425, 0.32450, 0.32508\n",
      "36, 4, 0.54206, 0.32453, 0.32512\n",
      "36, 0.32508\n",
      "36, 0.73695\n",
      "\n",
      "37, 0, 0.50731, 0.32577, 0.32610\n",
      "37, 1, 0.50241, 0.32581, 0.32609\n",
      "37, 2, 0.50464, 0.32581, 0.32608\n",
      "37, 3, 0.50172, 0.32582, 0.32609\n",
      "37, 4, 0.49806, 0.32583, 0.32609\n",
      "37, 0.32608\n",
      "37, 0.73719\n",
      "\n",
      "38, 0, 0.51185, 0.32580, 0.32620\n",
      "38, 1, 0.51174, 0.32580, 0.32619\n",
      "38, 2, 0.51044, 0.32580, 0.32618\n",
      "38, 3, 0.51243, 0.32581, 0.32625\n",
      "38, 4, 0.51090, 0.32581, 0.32619\n",
      "38, 0.32619\n",
      "38, 0.73702\n",
      "\n",
      "39, 0, 0.50036, 0.32583, 0.32614\n",
      "39, 1, 0.50500, 0.32584, 0.32616\n",
      "39, 2, 0.50542, 0.32585, 0.32616\n",
      "39, 3, 0.50247, 0.32585, 0.32615\n",
      "39, 4, 0.50109, 0.32586, 0.32615\n",
      "39, 0.32614\n",
      "39, 0.73705\n",
      "\n",
      "40, 0, 0.54910, 0.32312, 0.32369\n",
      "40, 1, 0.54847, 0.32315, 0.32368\n",
      "40, 2, 0.55005, 0.32315, 0.32367\n",
      "40, 3, 0.54927, 0.32316, 0.32367\n",
      "40, 4, 0.54913, 0.32316, 0.32367\n",
      "40, 0.32366\n",
      "40, 0.74325\n",
      "\n",
      "41, 0, 0.50808, 0.32581, 0.32617\n",
      "41, 1, 0.50889, 0.32582, 0.32620\n",
      "41, 2, 0.50550, 0.32582, 0.32618\n",
      "41, 3, 0.50523, 0.32582, 0.32617\n",
      "41, 4, 0.50281, 0.32583, 0.32617\n",
      "41, 0.32617\n",
      "41, 0.74320\n",
      "\n",
      "42, 0, 0.51451, 0.32572, 0.32633\n",
      "42, 1, 0.51218, 0.32572, 0.32627\n",
      "42, 2, 0.51225, 0.32573, 0.32630\n",
      "42, 3, 0.51231, 0.32573, 0.32623\n",
      "42, 4, 0.51168, 0.32575, 0.32613\n",
      "42, 0.32623\n",
      "42, 0.74299\n",
      "\n",
      "43, 0, 0.52453, 0.32531, 0.32556\n",
      "43, 1, 0.52540, 0.32532, 0.32555\n",
      "43, 2, 0.52549, 0.32532, 0.32555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43, 3, 0.52514, 0.32532, 0.32555\n",
      "43, 4, 0.52195, 0.32533, 0.32557\n",
      "43, 0.32555\n",
      "43, 0.74450\n",
      "\n",
      "44, 0, 0.53935, 0.32351, 0.32338\n",
      "44, 1, 0.54069, 0.32351, 0.32339\n",
      "44, 2, 0.54000, 0.32360, 0.32346\n",
      "44, 3, 0.53822, 0.32361, 0.32341\n",
      "44, 4, 0.53906, 0.32363, 0.32344\n",
      "44, 0.32339\n",
      "44, 0.75160\n",
      "\n",
      "45, 0, 0.51868, 0.32516, 0.32573\n",
      "45, 1, 0.51868, 0.32516, 0.32568\n",
      "45, 2, 0.51923, 0.32517, 0.32570\n",
      "45, 3, 0.51985, 0.32518, 0.32576\n",
      "45, 4, 0.51913, 0.32519, 0.32568\n",
      "45, 0.32569\n",
      "45, 0.75287\n",
      "\n",
      "46, 0, 0.50878, 0.32582, 0.32620\n",
      "46, 1, 0.50911, 0.32582, 0.32623\n",
      "46, 2, 0.50297, 0.32583, 0.32613\n",
      "46, 3, 0.50353, 0.32583, 0.32613\n",
      "46, 4, 0.50526, 0.32584, 0.32613\n",
      "46, 0.32612\n",
      "46, 0.75289\n",
      "\n",
      "47, 0, 0.51458, 0.32565, 0.32618\n",
      "47, 1, 0.51477, 0.32566, 0.32624\n",
      "47, 2, 0.51492, 0.32566, 0.32618\n",
      "47, 3, 0.51270, 0.32566, 0.32623\n",
      "47, 4, 0.51311, 0.32568, 0.32625\n",
      "47, 0.32620\n",
      "47, 0.75282\n",
      "\n",
      "48, 0, 0.53674, 0.32481, 0.32532\n",
      "48, 1, 0.53649, 0.32484, 0.32532\n",
      "48, 2, 0.53631, 0.32484, 0.32530\n",
      "48, 3, 0.53619, 0.32485, 0.32535\n",
      "48, 4, 0.53755, 0.32485, 0.32531\n",
      "48, 0.32530\n",
      "48, 0.75498\n",
      "\n",
      "49, 0, 0.52683, 0.32479, 0.32501\n",
      "49, 1, 0.52661, 0.32481, 0.32498\n",
      "49, 2, 0.52494, 0.32482, 0.32498\n",
      "49, 3, 0.52762, 0.32482, 0.32499\n",
      "49, 4, 0.52534, 0.32484, 0.32499\n",
      "49, 0.32498\n",
      "49, 0.75764\n",
      "\n",
      "50, 0, 0.51450, 0.32568, 0.32602\n",
      "50, 1, 0.51723, 0.32570, 0.32613\n",
      "50, 2, 0.51518, 0.32570, 0.32608\n",
      "50, 3, 0.51238, 0.32572, 0.32602\n",
      "50, 4, 0.51339, 0.32572, 0.32604\n",
      "50, 0.32603\n",
      "50, 0.75792\n",
      "\n",
      "51, 0, 0.52873, 0.32453, 0.32529\n",
      "51, 1, 0.53161, 0.32458, 0.32533\n",
      "51, 2, 0.52951, 0.32460, 0.32521\n",
      "51, 3, 0.52806, 0.32460, 0.32525\n",
      "51, 4, 0.52775, 0.32461, 0.32523\n",
      "51, 0.32523\n",
      "51, 0.75996\n",
      "\n",
      "52, 0, 0.52569, 0.32516, 0.32549\n",
      "52, 1, 0.52649, 0.32516, 0.32553\n",
      "52, 2, 0.52683, 0.32517, 0.32551\n",
      "52, 3, 0.52592, 0.32517, 0.32557\n",
      "52, 4, 0.52530, 0.32520, 0.32555\n",
      "52, 0.32552\n",
      "52, 0.76168\n",
      "\n",
      "53, 0, 0.55173, 0.32301, 0.32313\n",
      "53, 1, 0.54954, 0.32303, 0.32313\n",
      "53, 2, 0.55085, 0.32306, 0.32317\n",
      "53, 3, 0.54924, 0.32306, 0.32313\n",
      "53, 4, 0.55025, 0.32306, 0.32314\n",
      "53, 0.32313\n",
      "53, 0.76851\n",
      "\n",
      "54, 0, 0.51439, 0.32570, 0.32591\n",
      "54, 1, 0.51358, 0.32571, 0.32601\n",
      "54, 2, 0.51523, 0.32571, 0.32603\n",
      "54, 3, 0.51374, 0.32571, 0.32591\n",
      "54, 4, 0.51331, 0.32571, 0.32598\n",
      "54, 0.32594\n",
      "54, 0.76898\n",
      "\n",
      "55, 0, 0.51385, 0.32538, 0.32583\n",
      "55, 1, 0.51232, 0.32540, 0.32583\n",
      "55, 2, 0.51344, 0.32541, 0.32587\n",
      "55, 3, 0.51240, 0.32543, 0.32586\n",
      "55, 4, 0.51440, 0.32544, 0.32601\n",
      "55, 0.32585\n",
      "55, 0.76966\n",
      "\n",
      "56, 0, 0.52098, 0.32468, 0.32491\n",
      "56, 1, 0.52217, 0.32469, 0.32493\n",
      "56, 2, 0.52277, 0.32469, 0.32491\n",
      "56, 3, 0.52282, 0.32470, 0.32489\n",
      "56, 4, 0.52072, 0.32474, 0.32494\n",
      "56, 0.32490\n",
      "56, 0.77196\n",
      "\n",
      "57, 0, 0.51237, 0.32570, 0.32603\n",
      "57, 1, 0.51289, 0.32570, 0.32602\n",
      "57, 2, 0.51207, 0.32570, 0.32602\n",
      "57, 3, 0.51217, 0.32570, 0.32602\n",
      "57, 4, 0.51225, 0.32571, 0.32604\n",
      "57, 0.32603\n",
      "57, 0.77226\n",
      "\n",
      "58, 0, 0.53061, 0.32522, 0.32576\n",
      "58, 1, 0.53119, 0.32525, 0.32570\n",
      "58, 2, 0.53072, 0.32526, 0.32570\n",
      "58, 3, 0.53153, 0.32526, 0.32567\n",
      "58, 4, 0.53043, 0.32526, 0.32574\n",
      "58, 0.32570\n",
      "58, 0.77340\n",
      "\n",
      "59, 0, 0.50403, 0.32575, 0.32611\n",
      "59, 1, 0.50464, 0.32576, 0.32614\n",
      "59, 2, 0.50419, 0.32577, 0.32611\n",
      "59, 3, 0.50428, 0.32577, 0.32611\n",
      "59, 4, 0.50375, 0.32578, 0.32612\n",
      "59, 0.32611\n",
      "59, 0.77347\n",
      "\n",
      "60, 0, 0.51088, 0.32568, 0.32621\n",
      "60, 1, 0.51052, 0.32570, 0.32608\n",
      "60, 2, 0.50588, 0.32574, 0.32610\n",
      "60, 3, 0.51423, 0.32575, 0.32608\n",
      "60, 4, 0.51498, 0.32577, 0.32608\n",
      "60, 0.32608\n",
      "60, 0.77369\n",
      "\n",
      "61, 0, 0.50692, 0.32568, 0.32607\n",
      "61, 1, 0.50624, 0.32569, 0.32609\n",
      "61, 2, 0.50771, 0.32571, 0.32610\n",
      "61, 3, 0.50572, 0.32573, 0.32611\n",
      "61, 4, 0.51126, 0.32574, 0.32609\n",
      "61, 0.32607\n",
      "61, 0.77380\n",
      "\n",
      "62, 0, 0.51375, 0.32563, 0.32602\n",
      "62, 1, 0.51406, 0.32565, 0.32604\n",
      "62, 2, 0.51638, 0.32566, 0.32604\n",
      "62, 3, 0.51567, 0.32566, 0.32604\n",
      "62, 4, 0.51579, 0.32566, 0.32605\n",
      "62, 0.32603\n",
      "62, 0.77402\n",
      "\n",
      "63, 0, 0.51034, 0.32564, 0.32586\n",
      "63, 1, 0.50936, 0.32567, 0.32585\n",
      "63, 2, 0.50915, 0.32568, 0.32587\n",
      "63, 3, 0.51345, 0.32570, 0.32598\n",
      "63, 4, 0.51023, 0.32571, 0.32589\n",
      "63, 0.32586\n",
      "63, 0.77471\n",
      "\n",
      "64, 0, 0.51232, 0.32569, 0.32604\n",
      "64, 1, 0.51331, 0.32570, 0.32602\n",
      "64, 2, 0.51308, 0.32571, 0.32608\n",
      "64, 3, 0.51076, 0.32571, 0.32608\n",
      "64, 4, 0.51327, 0.32572, 0.32602\n",
      "64, 0.32604\n",
      "64, 0.77507\n",
      "\n",
      "65, 0, 0.51192, 0.32578, 0.32613\n",
      "65, 1, 0.51043, 0.32582, 0.32613\n",
      "65, 2, 0.50812, 0.32582, 0.32613\n",
      "65, 3, 0.51019, 0.32583, 0.32612\n",
      "65, 4, 0.51097, 0.32583, 0.32611\n",
      "65, 0.32611\n",
      "65, 0.77518\n",
      "\n",
      "66, 0, 0.52259, 0.32548, 0.32582\n",
      "66, 1, 0.52371, 0.32548, 0.32592\n",
      "66, 2, 0.52438, 0.32549, 0.32598\n",
      "66, 3, 0.52461, 0.32550, 0.32590\n",
      "66, 4, 0.52132, 0.32550, 0.32582\n",
      "66, 0.32587\n",
      "66, 0.77586\n",
      "\n",
      "67, 0, 0.54172, 0.32441, 0.32465\n",
      "67, 1, 0.54276, 0.32442, 0.32468\n",
      "67, 2, 0.54255, 0.32443, 0.32474\n",
      "67, 3, 0.54135, 0.32445, 0.32467\n",
      "67, 4, 0.54149, 0.32445, 0.32469\n",
      "67, 0.32467\n",
      "67, 0.77879\n",
      "\n",
      "68, 0, 0.50804, 0.32577, 0.32604\n",
      "68, 1, 0.50926, 0.32578, 0.32604\n",
      "68, 2, 0.50821, 0.32578, 0.32605\n",
      "68, 3, 0.50804, 0.32579, 0.32604\n",
      "68, 4, 0.50918, 0.32579, 0.32604\n",
      "68, 0.32604\n",
      "68, 0.77911\n",
      "\n",
      "69, 0, 0.50720, 0.32566, 0.32607\n",
      "69, 1, 0.50841, 0.32566, 0.32605\n",
      "69, 2, 0.50874, 0.32567, 0.32606\n",
      "69, 3, 0.50694, 0.32567, 0.32605\n",
      "69, 4, 0.51209, 0.32567, 0.32606\n",
      "69, 0.32606\n",
      "69, 0.77938\n",
      "\n",
      "70, 0, 0.53284, 0.32476, 0.32538\n",
      "70, 1, 0.53402, 0.32477, 0.32541\n",
      "70, 2, 0.53092, 0.32477, 0.32543\n",
      "70, 3, 0.53342, 0.32477, 0.32537\n",
      "70, 4, 0.52920, 0.32479, 0.32539\n",
      "70, 0.32538\n",
      "70, 0.78115\n",
      "\n",
      "71, 0, 0.53636, 0.32492, 0.32555\n",
      "71, 1, 0.53628, 0.32493, 0.32559\n",
      "71, 2, 0.53490, 0.32493, 0.32562\n",
      "71, 3, 0.53557, 0.32494, 0.32555\n",
      "71, 4, 0.53634, 0.32495, 0.32561\n",
      "71, 0.32556\n",
      "71, 0.78233\n",
      "\n",
      "72, 0, 0.51813, 0.32554, 0.32609\n",
      "72, 1, 0.51504, 0.32559, 0.32609\n",
      "72, 2, 0.51478, 0.32559, 0.32607\n",
      "72, 3, 0.51451, 0.32561, 0.32613\n",
      "72, 4, 0.51463, 0.32562, 0.32603\n",
      "72, 0.32607\n",
      "72, 0.78242\n",
      "\n",
      "73, 0, 0.50366, 0.32574, 0.32611\n",
      "73, 1, 0.50493, 0.32574, 0.32614\n",
      "73, 2, 0.50917, 0.32575, 0.32622\n",
      "73, 3, 0.50358, 0.32578, 0.32612\n",
      "73, 4, 0.50448, 0.32578, 0.32613\n",
      "73, 0.32612\n",
      "73, 0.78251\n",
      "\n",
      "74, 0, 0.51510, 0.32560, 0.32582\n",
      "74, 1, 0.51681, 0.32563, 0.32584\n",
      "74, 2, 0.51460, 0.32565, 0.32581\n",
      "74, 3, 0.51352, 0.32566, 0.32585\n",
      "74, 4, 0.51344, 0.32567, 0.32585\n",
      "74, 0.32582\n",
      "74, 0.78327\n",
      "\n",
      "75, 0, 0.53662, 0.32426, 0.32447\n",
      "75, 1, 0.53748, 0.32431, 0.32453\n",
      "75, 2, 0.53454, 0.32432, 0.32448\n",
      "75, 3, 0.53406, 0.32434, 0.32446\n",
      "75, 4, 0.53265, 0.32437, 0.32449\n",
      "75, 0.32446\n",
      "75, 0.78632\n",
      "\n",
      "76, 0, 0.56318, 0.32237, 0.32289\n",
      "76, 1, 0.56323, 0.32240, 0.32288\n",
      "76, 2, 0.56345, 0.32240, 0.32285\n",
      "76, 3, 0.56196, 0.32242, 0.32287\n",
      "76, 4, 0.56397, 0.32243, 0.32290\n",
      "76, 0.32287\n",
      "76, 0.79204\n",
      "\n",
      "77, 0, 0.52154, 0.32552, 0.32596\n",
      "77, 1, 0.51845, 0.32554, 0.32591\n",
      "77, 2, 0.52059, 0.32554, 0.32592\n",
      "77, 3, 0.52374, 0.32554, 0.32592\n",
      "77, 4, 0.51967, 0.32555, 0.32593\n",
      "77, 0.32591\n",
      "77, 0.79253\n",
      "\n",
      "78, 0, 0.54758, 0.32306, 0.32403\n",
      "78, 1, 0.54845, 0.32307, 0.32404\n",
      "78, 2, 0.54917, 0.32310, 0.32405\n",
      "78, 3, 0.54889, 0.32310, 0.32405\n",
      "78, 4, 0.54778, 0.32311, 0.32396\n",
      "78, 0.32401\n",
      "78, 0.79654\n",
      "\n",
      "79, 0, 0.50963, 0.32570, 0.32613\n",
      "79, 1, 0.50794, 0.32571, 0.32615\n",
      "79, 2, 0.50684, 0.32572, 0.32613\n",
      "79, 3, 0.50894, 0.32572, 0.32616\n",
      "79, 4, 0.50960, 0.32573, 0.32612\n",
      "79, 0.32613\n",
      "79, 0.79656\n",
      "\n",
      "80, 0, 0.55894, 0.32251, 0.32281\n",
      "80, 1, 0.55848, 0.32251, 0.32272\n",
      "80, 2, 0.55771, 0.32254, 0.32272\n",
      "80, 3, 0.55913, 0.32254, 0.32272\n",
      "80, 4, 0.55869, 0.32254, 0.32271\n",
      "80, 0.32272\n",
      "80, 0.80220\n",
      "\n",
      "81, 0, 0.56562, 0.32122, 0.32106\n",
      "81, 1, 0.56349, 0.32122, 0.32102\n",
      "81, 2, 0.56730, 0.32126, 0.32103\n",
      "81, 3, 0.56759, 0.32128, 0.32108\n",
      "81, 4, 0.56619, 0.32128, 0.32103\n",
      "81, 0.32102\n",
      "81, 0.81068\n",
      "\n",
      "82, 0, 0.52491, 0.32470, 0.32540\n",
      "82, 1, 0.52370, 0.32475, 0.32541\n",
      "82, 2, 0.52090, 0.32476, 0.32545\n",
      "82, 3, 0.52152, 0.32476, 0.32543\n",
      "82, 4, 0.52302, 0.32477, 0.32542\n",
      "82, 0.32541\n",
      "82, 0.81219\n",
      "\n",
      "83, 0, 0.52418, 0.32527, 0.32545\n",
      "83, 1, 0.52427, 0.32528, 0.32537\n",
      "83, 2, 0.52365, 0.32531, 0.32539\n",
      "83, 3, 0.52313, 0.32531, 0.32538\n",
      "83, 4, 0.52238, 0.32531, 0.32540\n",
      "83, 0.32538\n",
      "83, 0.81326\n",
      "\n",
      "84, 0, 0.50395, 0.32584, 0.32609\n",
      "84, 1, 0.50302, 0.32590, 0.32608\n",
      "84, 2, 0.50568, 0.32590, 0.32612\n",
      "84, 3, 0.50248, 0.32592, 0.32608\n",
      "84, 4, 0.50441, 0.32593, 0.32610\n",
      "84, 0.32608\n",
      "84, 0.81339\n",
      "\n",
      "85, 0, 0.52152, 0.32534, 0.32570\n",
      "85, 1, 0.52138, 0.32536, 0.32572\n",
      "85, 2, 0.52415, 0.32536, 0.32573\n",
      "85, 3, 0.52034, 0.32537, 0.32571\n",
      "85, 4, 0.52058, 0.32537, 0.32572\n",
      "85, 0.32571\n",
      "85, 0.81414\n",
      "\n",
      "86, 0, 0.53670, 0.32396, 0.32420\n",
      "86, 1, 0.53876, 0.32396, 0.32415\n",
      "86, 2, 0.53702, 0.32396, 0.32420\n",
      "86, 3, 0.53595, 0.32399, 0.32418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86, 4, 0.53586, 0.32399, 0.32420\n",
      "86, 0.32418\n",
      "86, 0.81718\n",
      "\n",
      "87, 0, 0.52738, 0.32498, 0.32517\n",
      "87, 1, 0.52950, 0.32499, 0.32517\n",
      "87, 2, 0.52806, 0.32501, 0.32514\n",
      "87, 3, 0.52776, 0.32504, 0.32515\n",
      "87, 4, 0.52743, 0.32506, 0.32519\n",
      "87, 0.32514\n",
      "87, 0.81859\n",
      "\n",
      "88, 0, 0.52370, 0.32526, 0.32572\n",
      "88, 1, 0.52385, 0.32526, 0.32572\n",
      "88, 2, 0.52378, 0.32526, 0.32572\n",
      "88, 3, 0.52370, 0.32527, 0.32572\n",
      "88, 4, 0.52324, 0.32528, 0.32573\n",
      "88, 0.32571\n",
      "88, 0.81913\n",
      "\n",
      "89, 0, 0.53573, 0.32452, 0.32499\n",
      "89, 1, 0.53615, 0.32454, 0.32497\n",
      "89, 2, 0.53557, 0.32456, 0.32500\n",
      "89, 3, 0.53669, 0.32456, 0.32496\n",
      "89, 4, 0.53534, 0.32458, 0.32496\n",
      "89, 0.32497\n",
      "89, 0.82081\n",
      "\n",
      "90, 0, 0.52550, 0.32515, 0.32538\n",
      "90, 1, 0.52499, 0.32518, 0.32533\n",
      "90, 2, 0.52499, 0.32518, 0.32534\n",
      "90, 3, 0.52620, 0.32520, 0.32539\n",
      "90, 4, 0.52503, 0.32520, 0.32534\n",
      "90, 0.32534\n",
      "90, 0.82205\n",
      "\n",
      "91, 0, 0.53470, 0.32486, 0.32501\n",
      "91, 1, 0.53498, 0.32486, 0.32502\n",
      "91, 2, 0.53465, 0.32486, 0.32501\n",
      "91, 3, 0.53341, 0.32490, 0.32500\n",
      "91, 4, 0.53491, 0.32490, 0.32499\n",
      "91, 0.32500\n",
      "91, 0.82381\n",
      "\n",
      "92, 0, 0.54280, 0.32362, 0.32406\n",
      "92, 1, 0.54165, 0.32363, 0.32407\n",
      "92, 2, 0.54362, 0.32364, 0.32408\n",
      "92, 3, 0.54404, 0.32364, 0.32406\n",
      "92, 4, 0.54223, 0.32365, 0.32407\n",
      "92, 0.32405\n",
      "92, 0.82702\n",
      "\n",
      "93, 0, 0.53308, 0.32476, 0.32533\n",
      "93, 1, 0.53423, 0.32478, 0.32531\n",
      "93, 2, 0.53192, 0.32479, 0.32531\n",
      "93, 3, 0.53380, 0.32479, 0.32532\n",
      "93, 4, 0.53040, 0.32479, 0.32528\n",
      "93, 0.32529\n",
      "93, 0.82813\n",
      "\n",
      "94, 0, 0.54073, 0.32405, 0.32413\n",
      "94, 1, 0.54205, 0.32405, 0.32413\n",
      "94, 2, 0.53874, 0.32409, 0.32414\n",
      "94, 3, 0.53837, 0.32412, 0.32413\n",
      "94, 4, 0.54059, 0.32413, 0.32415\n",
      "94, 0.32412\n",
      "94, 0.83074\n",
      "\n",
      "95, 0, 0.53115, 0.32499, 0.32507\n",
      "95, 1, 0.53239, 0.32500, 0.32507\n",
      "95, 2, 0.52862, 0.32500, 0.32511\n",
      "95, 3, 0.53180, 0.32502, 0.32508\n",
      "95, 4, 0.52924, 0.32502, 0.32508\n",
      "95, 0.32507\n",
      "95, 0.83219\n",
      "\n",
      "96, 0, 0.49797, 0.32586, 0.32616\n",
      "96, 1, 0.49412, 0.32588, 0.32616\n",
      "96, 2, 0.49219, 0.32589, 0.32617\n",
      "96, 3, 0.49424, 0.32591, 0.32618\n",
      "96, 4, 0.49583, 0.32591, 0.32619\n",
      "96, 0.32617\n",
      "96, 0.83217\n",
      "\n",
      "97, 0, 0.51850, 0.32538, 0.32589\n",
      "97, 1, 0.51681, 0.32541, 0.32579\n",
      "97, 2, 0.51603, 0.32541, 0.32579\n",
      "97, 3, 0.51491, 0.32543, 0.32581\n",
      "97, 4, 0.51420, 0.32545, 0.32588\n",
      "97, 0.32580\n",
      "97, 0.83283\n",
      "\n",
      "98, 0, 0.50906, 0.32577, 0.32617\n",
      "98, 1, 0.51131, 0.32578, 0.32616\n",
      "98, 2, 0.51071, 0.32579, 0.32614\n",
      "98, 3, 0.50962, 0.32579, 0.32614\n",
      "98, 4, 0.51045, 0.32579, 0.32614\n",
      "98, 0.32615\n",
      "98, 0.83282\n",
      "\n",
      "99, 0, 0.55090, 0.32337, 0.32359\n",
      "99, 1, 0.55148, 0.32339, 0.32358\n",
      "99, 2, 0.55000, 0.32339, 0.32357\n",
      "99, 3, 0.55064, 0.32339, 0.32357\n",
      "99, 4, 0.55126, 0.32341, 0.32363\n",
      "99, 0.32358\n",
      "99, 0.83638\n",
      "\n",
      "100, 0, 0.50606, 0.32583, 0.32617\n",
      "100, 1, 0.50255, 0.32584, 0.32617\n",
      "100, 2, 0.50236, 0.32584, 0.32617\n",
      "100, 3, 0.50200, 0.32585, 0.32617\n",
      "100, 4, 0.50067, 0.32586, 0.32618\n",
      "100, 0.32617\n",
      "100, 0.83634\n",
      "\n",
      "101, 0, 0.51327, 0.32574, 0.32610\n",
      "101, 1, 0.50964, 0.32578, 0.32612\n",
      "101, 2, 0.51374, 0.32579, 0.32610\n",
      "101, 3, 0.51550, 0.32580, 0.32610\n",
      "101, 4, 0.51038, 0.32580, 0.32617\n",
      "101, 0.32609\n",
      "101, 0.83628\n",
      "\n",
      "102, 0, 0.51688, 0.32545, 0.32557\n",
      "102, 1, 0.51844, 0.32545, 0.32556\n",
      "102, 2, 0.51730, 0.32547, 0.32554\n",
      "102, 3, 0.51535, 0.32549, 0.32556\n",
      "102, 4, 0.51653, 0.32550, 0.32562\n",
      "102, 0.32556\n",
      "102, 0.83706\n",
      "\n",
      "103, 0, 0.50407, 0.32584, 0.32615\n",
      "103, 1, 0.50305, 0.32584, 0.32615\n",
      "103, 2, 0.50025, 0.32584, 0.32615\n",
      "103, 3, 0.50041, 0.32585, 0.32616\n",
      "103, 4, 0.50120, 0.32585, 0.32616\n",
      "103, 0.32615\n",
      "103, 0.83706\n",
      "\n",
      "104, 0, 0.52680, 0.32545, 0.32576\n",
      "104, 1, 0.52682, 0.32545, 0.32577\n",
      "104, 2, 0.52614, 0.32547, 0.32577\n",
      "104, 3, 0.52623, 0.32549, 0.32579\n",
      "104, 4, 0.52702, 0.32551, 0.32583\n",
      "104, 0.32577\n",
      "104, 0.83756\n",
      "\n",
      "105, 0, 0.51876, 0.32553, 0.32579\n",
      "105, 1, 0.51957, 0.32554, 0.32581\n",
      "105, 2, 0.51906, 0.32555, 0.32578\n",
      "105, 3, 0.51736, 0.32556, 0.32578\n",
      "105, 4, 0.51790, 0.32557, 0.32579\n",
      "105, 0.32579\n",
      "105, 0.83808\n",
      "\n",
      "106, 0, 0.52951, 0.32517, 0.32540\n",
      "106, 1, 0.52803, 0.32518, 0.32540\n",
      "106, 2, 0.52686, 0.32519, 0.32542\n",
      "106, 3, 0.52483, 0.32519, 0.32545\n",
      "106, 4, 0.52672, 0.32520, 0.32540\n",
      "106, 0.32540\n",
      "106, 0.83909\n",
      "\n",
      "107, 0, 0.53928, 0.32416, 0.32469\n",
      "107, 1, 0.54023, 0.32417, 0.32470\n",
      "107, 2, 0.54011, 0.32418, 0.32465\n",
      "107, 3, 0.53920, 0.32423, 0.32466\n",
      "107, 4, 0.53980, 0.32423, 0.32468\n",
      "107, 0.32465\n",
      "107, 0.84106\n",
      "\n",
      "108, 0, 0.54513, 0.32405, 0.32412\n",
      "108, 1, 0.54258, 0.32405, 0.32416\n",
      "108, 2, 0.54305, 0.32405, 0.32411\n",
      "108, 3, 0.54499, 0.32406, 0.32420\n",
      "108, 4, 0.54285, 0.32407, 0.32419\n",
      "108, 0.32415\n",
      "108, 0.84367\n",
      "\n",
      "109, 0, 0.54486, 0.32275, 0.32306\n",
      "109, 1, 0.54313, 0.32276, 0.32305\n",
      "109, 2, 0.54312, 0.32276, 0.32306\n",
      "109, 3, 0.54207, 0.32279, 0.32304\n",
      "109, 4, 0.54418, 0.32279, 0.32313\n",
      "109, 0.32305\n",
      "109, 0.84714\n",
      "\n",
      "110, 0, 0.55286, 0.32285, 0.32269\n",
      "110, 1, 0.55158, 0.32286, 0.32270\n",
      "110, 2, 0.55138, 0.32287, 0.32270\n",
      "110, 3, 0.55389, 0.32287, 0.32269\n",
      "110, 4, 0.55169, 0.32287, 0.32270\n",
      "110, 0.32269\n",
      "110, 0.85089\n",
      "\n",
      "111, 0, 0.52672, 0.32511, 0.32564\n",
      "111, 1, 0.52684, 0.32512, 0.32561\n",
      "111, 2, 0.52736, 0.32513, 0.32561\n",
      "111, 3, 0.52587, 0.32514, 0.32559\n",
      "111, 4, 0.52837, 0.32514, 0.32563\n",
      "111, 0.32560\n",
      "111, 0.85152\n",
      "\n",
      "112, 0, 0.52844, 0.32516, 0.32559\n",
      "112, 1, 0.52923, 0.32518, 0.32559\n",
      "112, 2, 0.52767, 0.32521, 0.32558\n",
      "112, 3, 0.52846, 0.32521, 0.32559\n",
      "112, 4, 0.52710, 0.32522, 0.32560\n",
      "112, 0.32558\n",
      "112, 0.85225\n",
      "\n",
      "113, 0, 0.50496, 0.32562, 0.32589\n",
      "113, 1, 0.50464, 0.32563, 0.32592\n",
      "113, 2, 0.50617, 0.32563, 0.32591\n",
      "113, 3, 0.50885, 0.32565, 0.32604\n",
      "113, 4, 0.50683, 0.32565, 0.32593\n",
      "113, 0.32592\n",
      "113, 0.85256\n",
      "\n",
      "114, 0, 0.52073, 0.32533, 0.32561\n",
      "114, 1, 0.52002, 0.32533, 0.32562\n",
      "114, 2, 0.51967, 0.32534, 0.32562\n",
      "114, 3, 0.51840, 0.32535, 0.32564\n",
      "114, 4, 0.51958, 0.32535, 0.32564\n",
      "114, 0.32561\n",
      "114, 0.85328\n",
      "\n",
      "115, 0, 0.54204, 0.32451, 0.32457\n",
      "115, 1, 0.54231, 0.32452, 0.32458\n",
      "115, 2, 0.54021, 0.32455, 0.32456\n",
      "115, 3, 0.54017, 0.32456, 0.32452\n",
      "115, 4, 0.54043, 0.32457, 0.32456\n",
      "115, 0.32454\n",
      "115, 0.85494\n",
      "\n",
      "116, 0, 0.52065, 0.32555, 0.32576\n",
      "116, 1, 0.51894, 0.32557, 0.32574\n",
      "116, 2, 0.51678, 0.32559, 0.32583\n",
      "116, 3, 0.51755, 0.32559, 0.32575\n",
      "116, 4, 0.51903, 0.32559, 0.32582\n",
      "116, 0.32576\n",
      "116, 0.85536\n",
      "\n",
      "117, 0, 0.51835, 0.32568, 0.32619\n",
      "117, 1, 0.51784, 0.32569, 0.32619\n",
      "117, 2, 0.51848, 0.32569, 0.32627\n",
      "117, 3, 0.51570, 0.32571, 0.32618\n",
      "117, 4, 0.52012, 0.32571, 0.32636\n",
      "117, 0.32620\n",
      "117, 0.85533\n",
      "\n",
      "118, 0, 0.54535, 0.32448, 0.32473\n",
      "118, 1, 0.54297, 0.32449, 0.32474\n",
      "118, 2, 0.54312, 0.32450, 0.32478\n",
      "118, 3, 0.54567, 0.32452, 0.32475\n",
      "118, 4, 0.54263, 0.32452, 0.32476\n",
      "118, 0.32473\n",
      "118, 0.85690\n",
      "\n",
      "119, 0, 0.52703, 0.32489, 0.32496\n",
      "119, 1, 0.52760, 0.32491, 0.32495\n",
      "119, 2, 0.52724, 0.32492, 0.32498\n",
      "119, 3, 0.52541, 0.32494, 0.32495\n",
      "119, 4, 0.52589, 0.32497, 0.32501\n",
      "119, 0.32496\n",
      "119, 0.85816\n",
      "\n",
      "120, 0, 0.51174, 0.32574, 0.32601\n",
      "120, 1, 0.51052, 0.32575, 0.32602\n",
      "120, 2, 0.51096, 0.32576, 0.32600\n",
      "120, 3, 0.51100, 0.32576, 0.32599\n",
      "120, 4, 0.51097, 0.32576, 0.32601\n",
      "120, 0.32600\n",
      "120, 0.85833\n",
      "\n",
      "121, 0, 0.53260, 0.32473, 0.32481\n",
      "121, 1, 0.52900, 0.32474, 0.32482\n",
      "121, 2, 0.53275, 0.32476, 0.32481\n",
      "121, 3, 0.53283, 0.32476, 0.32483\n",
      "121, 4, 0.53086, 0.32477, 0.32484\n",
      "121, 0.32481\n",
      "121, 0.86004\n",
      "\n",
      "122, 0, 0.54350, 0.32404, 0.32463\n",
      "122, 1, 0.54378, 0.32405, 0.32457\n",
      "122, 2, 0.54414, 0.32406, 0.32459\n",
      "122, 3, 0.54317, 0.32406, 0.32458\n",
      "122, 4, 0.54251, 0.32406, 0.32464\n",
      "122, 0.32459\n",
      "122, 0.86189\n",
      "\n",
      "123, 0, 0.54519, 0.32395, 0.32418\n",
      "123, 1, 0.54453, 0.32401, 0.32417\n",
      "123, 2, 0.54339, 0.32401, 0.32420\n",
      "123, 3, 0.54468, 0.32401, 0.32420\n",
      "123, 4, 0.54445, 0.32404, 0.32418\n",
      "123, 0.32418\n",
      "123, 0.86373\n",
      "\n",
      "124, 0, 0.50630, 0.32584, 0.32617\n",
      "124, 1, 0.50653, 0.32585, 0.32615\n",
      "124, 2, 0.50465, 0.32585, 0.32616\n",
      "124, 3, 0.50314, 0.32586, 0.32616\n",
      "124, 4, 0.50167, 0.32587, 0.32617\n",
      "124, 0.32616\n",
      "124, 0.86372\n",
      "\n",
      "125, 0, 0.52771, 0.32505, 0.32558\n",
      "125, 1, 0.52899, 0.32506, 0.32558\n",
      "125, 2, 0.52705, 0.32510, 0.32558\n",
      "125, 3, 0.52738, 0.32510, 0.32558\n",
      "125, 4, 0.52609, 0.32511, 0.32558\n",
      "125, 0.32558\n",
      "125, 0.86429\n",
      "\n",
      "126, 0, 0.51027, 0.32582, 0.32617\n",
      "126, 1, 0.50785, 0.32583, 0.32617\n",
      "126, 2, 0.50805, 0.32584, 0.32619\n",
      "126, 3, 0.50440, 0.32584, 0.32617\n",
      "126, 4, 0.50332, 0.32584, 0.32617\n",
      "126, 0.32617\n",
      "126, 0.86426\n",
      "\n",
      "127, 0, 0.53668, 0.32469, 0.32478\n",
      "127, 1, 0.53614, 0.32472, 0.32480\n",
      "127, 2, 0.53745, 0.32475, 0.32483\n",
      "127, 3, 0.53515, 0.32476, 0.32481\n",
      "127, 4, 0.53565, 0.32476, 0.32487\n",
      "127, 0.32479\n",
      "127, 0.86583\n",
      "\n",
      "128, 0, 0.53319, 0.32501, 0.32558\n",
      "128, 1, 0.53104, 0.32502, 0.32557\n",
      "128, 2, 0.53306, 0.32502, 0.32557\n",
      "128, 3, 0.53049, 0.32504, 0.32560\n",
      "128, 4, 0.53034, 0.32505, 0.32560\n",
      "128, 0.32557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128, 0.86654\n",
      "\n",
      "129, 0, 0.51190, 0.32572, 0.32612\n",
      "129, 1, 0.50835, 0.32574, 0.32620\n",
      "129, 2, 0.50878, 0.32574, 0.32619\n",
      "129, 3, 0.50566, 0.32578, 0.32615\n",
      "129, 4, 0.50566, 0.32579, 0.32614\n",
      "129, 0.32614\n",
      "129, 0.86655\n",
      "\n",
      "130, 0, 0.52678, 0.32471, 0.32535\n",
      "130, 1, 0.52933, 0.32472, 0.32539\n",
      "130, 2, 0.53022, 0.32473, 0.32541\n",
      "130, 3, 0.52634, 0.32474, 0.32536\n",
      "130, 4, 0.52570, 0.32476, 0.32543\n",
      "130, 0.32536\n",
      "130, 0.86756\n",
      "\n",
      "131, 0, 0.53191, 0.32511, 0.32556\n",
      "131, 1, 0.52971, 0.32512, 0.32550\n",
      "131, 2, 0.52847, 0.32515, 0.32553\n",
      "131, 3, 0.52856, 0.32517, 0.32549\n",
      "131, 4, 0.52644, 0.32517, 0.32546\n",
      "131, 0.32548\n",
      "131, 0.86827\n",
      "\n",
      "132, 0, 0.51803, 0.32540, 0.32560\n",
      "132, 1, 0.51895, 0.32540, 0.32557\n",
      "132, 2, 0.51769, 0.32545, 0.32563\n",
      "132, 3, 0.51677, 0.32547, 0.32560\n",
      "132, 4, 0.51564, 0.32548, 0.32559\n",
      "132, 0.32558\n",
      "132, 0.86887\n",
      "\n",
      "133, 0, 0.54437, 0.32385, 0.32411\n",
      "133, 1, 0.54474, 0.32387, 0.32413\n",
      "133, 2, 0.54458, 0.32387, 0.32412\n",
      "133, 3, 0.54552, 0.32388, 0.32414\n",
      "133, 4, 0.54459, 0.32388, 0.32412\n",
      "133, 0.32412\n",
      "133, 0.87098\n",
      "\n",
      "134, 0, 0.51385, 0.32536, 0.32567\n",
      "134, 1, 0.51461, 0.32540, 0.32573\n",
      "134, 2, 0.51321, 0.32542, 0.32572\n",
      "134, 3, 0.51184, 0.32543, 0.32574\n",
      "134, 4, 0.51647, 0.32545, 0.32589\n",
      "134, 0.32570\n",
      "134, 0.87125\n",
      "\n",
      "135, 0, 0.52944, 0.32475, 0.32515\n",
      "135, 1, 0.53072, 0.32475, 0.32518\n",
      "135, 2, 0.53089, 0.32476, 0.32511\n",
      "135, 3, 0.52733, 0.32479, 0.32510\n",
      "135, 4, 0.52979, 0.32480, 0.32513\n",
      "135, 0.32511\n",
      "135, 0.87218\n",
      "\n",
      "136, 0, 0.50644, 0.32582, 0.32617\n",
      "136, 1, 0.50436, 0.32584, 0.32616\n",
      "136, 2, 0.50342, 0.32585, 0.32616\n",
      "136, 3, 0.50197, 0.32586, 0.32617\n",
      "136, 4, 0.50273, 0.32586, 0.32617\n",
      "136, 0.32616\n",
      "136, 0.87218\n",
      "\n",
      "137, 0, 0.53160, 0.32472, 0.32547\n",
      "137, 1, 0.53118, 0.32473, 0.32546\n",
      "137, 2, 0.53130, 0.32475, 0.32554\n",
      "137, 3, 0.53116, 0.32477, 0.32551\n",
      "137, 4, 0.52914, 0.32477, 0.32548\n",
      "137, 0.32547\n",
      "137, 0.87290\n",
      "\n",
      "138, 0, 0.52810, 0.32516, 0.32592\n",
      "138, 1, 0.52839, 0.32516, 0.32594\n",
      "138, 2, 0.52964, 0.32517, 0.32591\n",
      "138, 3, 0.52871, 0.32518, 0.32595\n",
      "138, 4, 0.52889, 0.32518, 0.32593\n",
      "138, 0.32591\n",
      "138, 0.87311\n",
      "\n",
      "139, 0, 0.57738, 0.32048, 0.32139\n",
      "139, 1, 0.57853, 0.32048, 0.32145\n",
      "139, 2, 0.57784, 0.32050, 0.32144\n",
      "139, 3, 0.57781, 0.32053, 0.32144\n",
      "139, 4, 0.57736, 0.32053, 0.32136\n",
      "139, 0.32140\n",
      "139, 0.87741\n",
      "\n",
      "140, 0, 0.51173, 0.32574, 0.32607\n",
      "140, 1, 0.51203, 0.32577, 0.32606\n",
      "140, 2, 0.51163, 0.32578, 0.32604\n",
      "140, 3, 0.51214, 0.32578, 0.32606\n",
      "140, 4, 0.51236, 0.32580, 0.32606\n",
      "140, 0.32605\n",
      "140, 0.87750\n",
      "\n",
      "141, 0, 0.53162, 0.32427, 0.32503\n",
      "141, 1, 0.53365, 0.32432, 0.32505\n",
      "141, 2, 0.53343, 0.32432, 0.32508\n",
      "141, 3, 0.53190, 0.32434, 0.32502\n",
      "141, 4, 0.53006, 0.32434, 0.32502\n",
      "141, 0.32502\n",
      "141, 0.87856\n",
      "\n",
      "142, 0, 0.51618, 0.32545, 0.32588\n",
      "142, 1, 0.51334, 0.32550, 0.32581\n",
      "142, 2, 0.51590, 0.32550, 0.32589\n",
      "142, 3, 0.51526, 0.32550, 0.32585\n",
      "142, 4, 0.51332, 0.32551, 0.32583\n",
      "142, 0.32583\n",
      "142, 0.87883\n",
      "\n",
      "143, 0, 0.50544, 0.32577, 0.32607\n",
      "143, 1, 0.50782, 0.32579, 0.32610\n",
      "143, 2, 0.50450, 0.32581, 0.32607\n",
      "143, 3, 0.50656, 0.32582, 0.32611\n",
      "143, 4, 0.50242, 0.32582, 0.32605\n",
      "143, 0.32605\n",
      "143, 0.87892\n",
      "\n",
      "144, 0, 0.51852, 0.32545, 0.32591\n",
      "144, 1, 0.51434, 0.32550, 0.32587\n",
      "144, 2, 0.51344, 0.32553, 0.32587\n",
      "144, 3, 0.51544, 0.32553, 0.32595\n",
      "144, 4, 0.51418, 0.32554, 0.32587\n",
      "144, 0.32588\n",
      "144, 0.87923\n",
      "\n",
      "145, 0, 0.52240, 0.32506, 0.32516\n",
      "145, 1, 0.52344, 0.32506, 0.32516\n",
      "145, 2, 0.52315, 0.32508, 0.32515\n",
      "145, 3, 0.52240, 0.32508, 0.32516\n",
      "145, 4, 0.52252, 0.32508, 0.32516\n",
      "145, 0.32515\n",
      "145, 0.88004\n",
      "\n",
      "146, 0, 0.55757, 0.32263, 0.32304\n",
      "146, 1, 0.55667, 0.32263, 0.32300\n",
      "146, 2, 0.55629, 0.32267, 0.32300\n",
      "146, 3, 0.55747, 0.32268, 0.32307\n",
      "146, 4, 0.55609, 0.32268, 0.32301\n",
      "146, 0.32301\n",
      "146, 0.88271\n",
      "\n",
      "147, 0, 0.53973, 0.32363, 0.32411\n",
      "147, 1, 0.53940, 0.32363, 0.32411\n",
      "147, 2, 0.53858, 0.32363, 0.32409\n",
      "147, 3, 0.53941, 0.32366, 0.32410\n",
      "147, 4, 0.53897, 0.32366, 0.32410\n",
      "147, 0.32409\n",
      "147, 0.88443\n",
      "\n",
      "148, 0, 0.55111, 0.32350, 0.32406\n",
      "148, 1, 0.55032, 0.32351, 0.32406\n",
      "148, 2, 0.55126, 0.32352, 0.32406\n",
      "148, 3, 0.55214, 0.32353, 0.32407\n",
      "148, 4, 0.55233, 0.32355, 0.32406\n",
      "148, 0.32406\n",
      "148, 0.88605\n",
      "\n",
      "149, 0, 0.55834, 0.32391, 0.32460\n",
      "149, 1, 0.55733, 0.32394, 0.32485\n",
      "149, 2, 0.55730, 0.32395, 0.32459\n",
      "149, 3, 0.55759, 0.32396, 0.32468\n",
      "149, 4, 0.55670, 0.32398, 0.32454\n",
      "149, 0.32458\n",
      "149, 0.88741\n",
      "\n",
      "150, 0, 0.51850, 0.32532, 0.32553\n",
      "150, 1, 0.51533, 0.32534, 0.32551\n",
      "150, 2, 0.51808, 0.32539, 0.32556\n",
      "150, 3, 0.51681, 0.32541, 0.32560\n",
      "150, 4, 0.51573, 0.32541, 0.32558\n",
      "150, 0.32552\n",
      "150, 0.88801\n",
      "\n",
      "151, 0, 0.52491, 0.32506, 0.32545\n",
      "151, 1, 0.52476, 0.32506, 0.32546\n",
      "151, 2, 0.52433, 0.32507, 0.32542\n",
      "151, 3, 0.52260, 0.32507, 0.32548\n",
      "151, 4, 0.52552, 0.32508, 0.32540\n",
      "151, 0.32542\n",
      "151, 0.88850\n",
      "\n",
      "152, 0, 0.50550, 0.32572, 0.32602\n",
      "152, 1, 0.50396, 0.32572, 0.32600\n",
      "152, 2, 0.50737, 0.32575, 0.32605\n",
      "152, 3, 0.50276, 0.32576, 0.32601\n",
      "152, 4, 0.50771, 0.32576, 0.32606\n",
      "152, 0.32601\n",
      "152, 0.88860\n",
      "\n",
      "153, 0, 0.49871, 0.32585, 0.32613\n",
      "153, 1, 0.49895, 0.32585, 0.32613\n",
      "153, 2, 0.49805, 0.32585, 0.32614\n",
      "153, 3, 0.49924, 0.32585, 0.32613\n",
      "153, 4, 0.49747, 0.32586, 0.32613\n",
      "153, 0.32613\n",
      "153, 0.88862\n",
      "\n",
      "154, 0, 0.54126, 0.32387, 0.32409\n",
      "154, 1, 0.54121, 0.32391, 0.32410\n",
      "154, 2, 0.54227, 0.32391, 0.32408\n",
      "154, 3, 0.54327, 0.32392, 0.32412\n",
      "154, 4, 0.54166, 0.32392, 0.32410\n",
      "154, 0.32408\n",
      "154, 0.88994\n",
      "\n",
      "155, 0, 0.53202, 0.32444, 0.32467\n",
      "155, 1, 0.52983, 0.32445, 0.32465\n",
      "155, 2, 0.52928, 0.32446, 0.32467\n",
      "155, 3, 0.53171, 0.32448, 0.32464\n",
      "155, 4, 0.52971, 0.32450, 0.32471\n",
      "155, 0.32466\n",
      "155, 0.89113\n",
      "\n",
      "156, 0, 0.51911, 0.32555, 0.32590\n",
      "156, 1, 0.51594, 0.32557, 0.32587\n",
      "156, 2, 0.51566, 0.32559, 0.32586\n",
      "156, 3, 0.51692, 0.32559, 0.32587\n",
      "156, 4, 0.51606, 0.32561, 0.32588\n",
      "156, 0.32586\n",
      "156, 0.89138\n",
      "\n",
      "157, 0, 0.53104, 0.32454, 0.32505\n",
      "157, 1, 0.52920, 0.32456, 0.32505\n",
      "157, 2, 0.52691, 0.32459, 0.32508\n",
      "157, 3, 0.52584, 0.32466, 0.32509\n",
      "157, 4, 0.52538, 0.32467, 0.32511\n",
      "157, 0.32506\n",
      "157, 0.89226\n",
      "\n",
      "158, 0, 0.50210, 0.32583, 0.32615\n",
      "158, 1, 0.50198, 0.32586, 0.32618\n",
      "158, 2, 0.49896, 0.32586, 0.32616\n",
      "158, 3, 0.49948, 0.32587, 0.32617\n",
      "158, 4, 0.49870, 0.32587, 0.32616\n",
      "158, 0.32616\n",
      "158, 0.89227\n",
      "\n",
      "159, 0, 0.52022, 0.32560, 0.32609\n",
      "159, 1, 0.51873, 0.32561, 0.32604\n",
      "159, 2, 0.52052, 0.32561, 0.32613\n",
      "159, 3, 0.51894, 0.32564, 0.32605\n",
      "159, 4, 0.51800, 0.32565, 0.32605\n",
      "159, 0.32606\n",
      "159, 0.89234\n",
      "\n",
      "160, 0, 0.50794, 0.32574, 0.32612\n",
      "160, 1, 0.50691, 0.32575, 0.32613\n",
      "160, 2, 0.50589, 0.32575, 0.32612\n",
      "160, 3, 0.50666, 0.32575, 0.32612\n",
      "160, 4, 0.50641, 0.32576, 0.32613\n",
      "160, 0.32612\n",
      "160, 0.89238\n",
      "\n",
      "161, 0, 0.50563, 0.32584, 0.32616\n",
      "161, 1, 0.50284, 0.32584, 0.32615\n",
      "161, 2, 0.50318, 0.32584, 0.32616\n",
      "161, 3, 0.49996, 0.32585, 0.32616\n",
      "161, 4, 0.50671, 0.32585, 0.32615\n",
      "161, 0.32615\n",
      "161, 0.89238\n",
      "\n",
      "162, 0, 0.53587, 0.32447, 0.32534\n",
      "162, 1, 0.53669, 0.32450, 0.32536\n",
      "162, 2, 0.53862, 0.32454, 0.32527\n",
      "162, 3, 0.53314, 0.32456, 0.32533\n",
      "162, 4, 0.53402, 0.32456, 0.32532\n",
      "162, 0.32530\n",
      "162, 0.89298\n",
      "\n",
      "163, 0, 0.53263, 0.32447, 0.32491\n",
      "163, 1, 0.53196, 0.32447, 0.32492\n",
      "163, 2, 0.53244, 0.32448, 0.32492\n",
      "163, 3, 0.53266, 0.32449, 0.32497\n",
      "163, 4, 0.53301, 0.32449, 0.32493\n",
      "163, 0.32491\n",
      "163, 0.89394\n",
      "\n",
      "164, 0, 0.53470, 0.32387, 0.32387\n",
      "164, 1, 0.53325, 0.32387, 0.32391\n",
      "164, 2, 0.53409, 0.32388, 0.32386\n",
      "164, 3, 0.53471, 0.32390, 0.32385\n",
      "164, 4, 0.53288, 0.32391, 0.32384\n",
      "164, 0.32386\n",
      "164, 0.89546\n",
      "\n",
      "165, 0, 0.55147, 0.32335, 0.32352\n",
      "165, 1, 0.55116, 0.32335, 0.32355\n",
      "165, 2, 0.55173, 0.32337, 0.32363\n",
      "165, 3, 0.55232, 0.32338, 0.32363\n",
      "165, 4, 0.55161, 0.32339, 0.32362\n",
      "165, 0.32357\n",
      "165, 0.89749\n",
      "\n",
      "166, 0, 0.55385, 0.32365, 0.32364\n",
      "166, 1, 0.55301, 0.32366, 0.32363\n",
      "166, 2, 0.55298, 0.32366, 0.32363\n",
      "166, 3, 0.55340, 0.32367, 0.32368\n",
      "166, 4, 0.55196, 0.32370, 0.32367\n",
      "166, 0.32363\n",
      "166, 0.89931\n",
      "\n",
      "167, 0, 0.52687, 0.32513, 0.32525\n",
      "167, 1, 0.52575, 0.32516, 0.32523\n",
      "167, 2, 0.52466, 0.32516, 0.32523\n",
      "167, 3, 0.52698, 0.32521, 0.32532\n",
      "167, 4, 0.52436, 0.32521, 0.32523\n",
      "167, 0.32523\n",
      "167, 0.89997\n",
      "\n",
      "168, 0, 0.52554, 0.32530, 0.32596\n",
      "168, 1, 0.52629, 0.32534, 0.32592\n",
      "168, 2, 0.52462, 0.32534, 0.32591\n",
      "168, 3, 0.52422, 0.32536, 0.32593\n",
      "168, 4, 0.52442, 0.32536, 0.32595\n",
      "168, 0.32592\n",
      "168, 0.90021\n",
      "\n",
      "169, 0, 0.55248, 0.32394, 0.32463\n",
      "169, 1, 0.55170, 0.32394, 0.32465\n",
      "169, 2, 0.55314, 0.32394, 0.32467\n",
      "169, 3, 0.55326, 0.32396, 0.32463\n",
      "169, 4, 0.55333, 0.32398, 0.32460\n",
      "169, 0.32462\n",
      "169, 0.90139\n",
      "\n",
      "170, 0, 0.53506, 0.32407, 0.32390\n",
      "170, 1, 0.53428, 0.32408, 0.32390\n",
      "170, 2, 0.53617, 0.32409, 0.32390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170, 3, 0.53618, 0.32412, 0.32387\n",
      "170, 4, 0.53629, 0.32413, 0.32387\n",
      "170, 0.32387\n",
      "170, 0.90286\n",
      "\n",
      "171, 0, 0.51439, 0.32559, 0.32602\n",
      "171, 1, 0.50740, 0.32560, 0.32590\n",
      "171, 2, 0.51009, 0.32561, 0.32590\n",
      "171, 3, 0.51131, 0.32568, 0.32597\n",
      "171, 4, 0.50805, 0.32568, 0.32595\n",
      "171, 0.32593\n",
      "171, 0.90298\n",
      "\n",
      "172, 0, 0.53563, 0.32439, 0.32476\n",
      "172, 1, 0.53417, 0.32442, 0.32474\n",
      "172, 2, 0.53523, 0.32445, 0.32479\n",
      "172, 3, 0.53404, 0.32445, 0.32475\n",
      "172, 4, 0.53343, 0.32446, 0.32476\n",
      "172, 0.32475\n",
      "172, 0.90403\n",
      "\n",
      "173, 0, 0.54065, 0.32439, 0.32470\n",
      "173, 1, 0.53943, 0.32441, 0.32467\n",
      "173, 2, 0.53940, 0.32443, 0.32470\n",
      "173, 3, 0.53938, 0.32443, 0.32471\n",
      "173, 4, 0.53952, 0.32445, 0.32475\n",
      "173, 0.32469\n",
      "173, 0.90500\n",
      "\n",
      "174, 0, 0.55441, 0.32240, 0.32258\n",
      "174, 1, 0.55361, 0.32240, 0.32259\n",
      "174, 2, 0.55452, 0.32241, 0.32259\n",
      "174, 3, 0.55337, 0.32242, 0.32260\n",
      "174, 4, 0.55277, 0.32243, 0.32257\n",
      "174, 0.32257\n",
      "174, 0.90696\n",
      "\n",
      "175, 0, 0.51657, 0.32546, 0.32572\n",
      "175, 1, 0.51660, 0.32547, 0.32573\n",
      "175, 2, 0.51893, 0.32548, 0.32574\n",
      "175, 3, 0.51870, 0.32550, 0.32580\n",
      "175, 4, 0.51687, 0.32551, 0.32573\n",
      "175, 0.32572\n",
      "175, 0.90730\n",
      "\n",
      "176, 0, 0.50319, 0.32589, 0.32614\n",
      "176, 1, 0.50188, 0.32590, 0.32614\n",
      "176, 2, 0.50369, 0.32591, 0.32613\n",
      "176, 3, 0.51278, 0.32592, 0.32620\n",
      "176, 4, 0.50360, 0.32596, 0.32614\n",
      "176, 0.32612\n",
      "176, 0.90732\n",
      "\n",
      "177, 0, 0.53339, 0.32420, 0.32447\n",
      "177, 1, 0.53288, 0.32422, 0.32447\n",
      "177, 2, 0.53477, 0.32423, 0.32452\n",
      "177, 3, 0.53293, 0.32426, 0.32451\n",
      "177, 4, 0.53353, 0.32427, 0.32450\n",
      "177, 0.32448\n",
      "177, 0.90836\n",
      "\n",
      "178, 0, 0.52579, 0.32526, 0.32585\n",
      "178, 1, 0.52643, 0.32526, 0.32586\n",
      "178, 2, 0.52368, 0.32528, 0.32590\n",
      "178, 3, 0.52503, 0.32529, 0.32588\n",
      "178, 4, 0.52651, 0.32530, 0.32588\n",
      "178, 0.32584\n",
      "178, 0.90856\n",
      "\n",
      "179, 0, 0.54478, 0.32276, 0.32365\n",
      "179, 1, 0.54523, 0.32276, 0.32363\n",
      "179, 2, 0.54443, 0.32279, 0.32363\n",
      "179, 3, 0.54606, 0.32280, 0.32368\n",
      "179, 4, 0.54456, 0.32280, 0.32362\n",
      "179, 0.32361\n",
      "179, 0.90978\n",
      "\n",
      "180, 0, 0.53053, 0.32481, 0.32509\n",
      "180, 1, 0.52822, 0.32483, 0.32509\n",
      "180, 2, 0.53075, 0.32483, 0.32523\n",
      "180, 3, 0.53232, 0.32484, 0.32514\n",
      "180, 4, 0.53028, 0.32484, 0.32515\n",
      "180, 0.32511\n",
      "180, 0.91048\n",
      "\n",
      "181, 0, 0.51181, 0.32580, 0.32604\n",
      "181, 1, 0.50787, 0.32583, 0.32606\n",
      "181, 2, 0.50789, 0.32583, 0.32604\n",
      "181, 3, 0.50721, 0.32586, 0.32604\n",
      "181, 4, 0.50711, 0.32587, 0.32603\n",
      "181, 0.32603\n",
      "181, 0.91056\n",
      "\n",
      "182, 0, 0.51125, 0.32565, 0.32612\n",
      "182, 1, 0.51326, 0.32566, 0.32612\n",
      "182, 2, 0.51340, 0.32567, 0.32618\n",
      "182, 3, 0.51457, 0.32567, 0.32616\n",
      "182, 4, 0.51325, 0.32567, 0.32614\n",
      "182, 0.32612\n",
      "182, 0.91060\n",
      "\n",
      "183, 0, 0.50583, 0.32582, 0.32616\n",
      "183, 1, 0.50822, 0.32582, 0.32616\n",
      "183, 2, 0.50449, 0.32582, 0.32615\n",
      "183, 3, 0.50659, 0.32582, 0.32617\n",
      "183, 4, 0.50389, 0.32582, 0.32616\n",
      "183, 0.32616\n",
      "183, 0.91061\n",
      "\n",
      "184, 0, 0.54892, 0.32369, 0.32389\n",
      "184, 1, 0.54893, 0.32370, 0.32391\n",
      "184, 2, 0.54786, 0.32371, 0.32389\n",
      "184, 3, 0.54778, 0.32372, 0.32392\n",
      "184, 4, 0.54637, 0.32373, 0.32393\n",
      "184, 0.32389\n",
      "184, 0.91205\n",
      "\n",
      "185, 0, 0.49808, 0.32585, 0.32618\n",
      "185, 1, 0.49734, 0.32586, 0.32618\n",
      "185, 2, 0.50113, 0.32588, 0.32618\n",
      "185, 3, 0.49807, 0.32588, 0.32617\n",
      "185, 4, 0.49990, 0.32588, 0.32619\n",
      "185, 0.32618\n",
      "185, 0.91203\n",
      "\n",
      "186, 0, 0.53048, 0.32517, 0.32552\n",
      "186, 1, 0.53013, 0.32519, 0.32552\n",
      "186, 2, 0.53037, 0.32521, 0.32553\n",
      "186, 3, 0.52946, 0.32521, 0.32553\n",
      "186, 4, 0.52846, 0.32522, 0.32553\n",
      "186, 0.32552\n",
      "186, 0.91235\n",
      "\n",
      "187, 0, 0.51514, 0.32529, 0.32568\n",
      "187, 1, 0.51445, 0.32531, 0.32572\n",
      "187, 2, 0.51412, 0.32532, 0.32571\n",
      "187, 3, 0.51248, 0.32533, 0.32571\n",
      "187, 4, 0.51467, 0.32533, 0.32575\n",
      "187, 0.32569\n",
      "187, 0.91274\n",
      "\n",
      "188, 0, 0.53045, 0.32448, 0.32488\n",
      "188, 1, 0.53190, 0.32450, 0.32488\n",
      "188, 2, 0.52892, 0.32450, 0.32490\n",
      "188, 3, 0.53116, 0.32452, 0.32492\n",
      "188, 4, 0.52853, 0.32454, 0.32488\n",
      "188, 0.32487\n",
      "188, 0.91361\n",
      "\n",
      "189, 0, 0.51371, 0.32572, 0.32616\n",
      "189, 1, 0.51783, 0.32572, 0.32612\n",
      "189, 2, 0.51628, 0.32573, 0.32612\n",
      "189, 3, 0.51471, 0.32573, 0.32612\n",
      "189, 4, 0.51258, 0.32574, 0.32617\n",
      "189, 0.32612\n",
      "189, 0.91359\n",
      "\n",
      "190, 0, 0.55059, 0.32325, 0.32372\n",
      "190, 1, 0.54908, 0.32329, 0.32364\n",
      "190, 2, 0.54941, 0.32329, 0.32362\n",
      "190, 3, 0.55003, 0.32330, 0.32361\n",
      "190, 4, 0.54846, 0.32332, 0.32357\n",
      "190, 0.32361\n",
      "190, 0.91502\n",
      "\n",
      "191, 0, 0.54539, 0.32372, 0.32431\n",
      "191, 1, 0.54600, 0.32377, 0.32441\n",
      "191, 2, 0.54510, 0.32379, 0.32428\n",
      "191, 3, 0.54463, 0.32379, 0.32430\n",
      "191, 4, 0.54414, 0.32380, 0.32432\n",
      "191, 0.32429\n",
      "191, 0.91604\n",
      "\n",
      "192, 0, 0.54012, 0.32494, 0.32506\n",
      "192, 1, 0.54082, 0.32494, 0.32507\n",
      "192, 2, 0.53847, 0.32497, 0.32510\n",
      "192, 3, 0.53881, 0.32497, 0.32510\n",
      "192, 4, 0.53952, 0.32499, 0.32509\n",
      "192, 0.32508\n",
      "192, 0.91671\n",
      "\n",
      "193, 0, 0.51770, 0.32540, 0.32591\n",
      "193, 1, 0.51652, 0.32541, 0.32591\n",
      "193, 2, 0.51743, 0.32541, 0.32593\n",
      "193, 3, 0.51843, 0.32542, 0.32591\n",
      "193, 4, 0.51654, 0.32542, 0.32597\n",
      "193, 0.32591\n",
      "193, 0.91689\n",
      "\n",
      "194, 0, 0.51331, 0.32563, 0.32583\n",
      "194, 1, 0.51259, 0.32564, 0.32576\n",
      "194, 2, 0.51045, 0.32565, 0.32579\n",
      "194, 3, 0.51383, 0.32566, 0.32584\n",
      "194, 4, 0.51119, 0.32569, 0.32571\n",
      "194, 0.32576\n",
      "194, 0.91709\n",
      "\n",
      "195, 0, 0.53394, 0.32496, 0.32571\n",
      "195, 1, 0.52934, 0.32503, 0.32559\n",
      "195, 2, 0.53145, 0.32503, 0.32559\n",
      "195, 3, 0.53218, 0.32503, 0.32558\n",
      "195, 4, 0.53248, 0.32504, 0.32561\n",
      "195, 0.32559\n",
      "195, 0.91744\n",
      "\n",
      "196, 0, 0.51823, 0.32515, 0.32557\n",
      "196, 1, 0.51497, 0.32518, 0.32558\n",
      "196, 2, 0.51527, 0.32519, 0.32554\n",
      "196, 3, 0.51397, 0.32521, 0.32550\n",
      "196, 4, 0.51597, 0.32522, 0.32563\n",
      "196, 0.32554\n",
      "196, 0.91769\n",
      "\n",
      "197, 0, 0.53865, 0.32465, 0.32526\n",
      "197, 1, 0.53788, 0.32467, 0.32522\n",
      "197, 2, 0.53800, 0.32467, 0.32521\n",
      "197, 3, 0.53816, 0.32468, 0.32523\n",
      "197, 4, 0.53746, 0.32470, 0.32524\n",
      "197, 0.32522\n",
      "197, 0.91823\n",
      "\n",
      "198, 0, 0.54125, 0.32383, 0.32359\n",
      "198, 1, 0.54418, 0.32386, 0.32352\n",
      "198, 2, 0.54489, 0.32387, 0.32351\n",
      "198, 3, 0.54261, 0.32388, 0.32354\n",
      "198, 4, 0.54395, 0.32391, 0.32351\n",
      "198, 0.32351\n",
      "198, 0.91978\n",
      "\n",
      "199, 0, 0.53576, 0.32487, 0.32562\n",
      "199, 1, 0.53487, 0.32491, 0.32561\n",
      "199, 2, 0.53430, 0.32493, 0.32561\n",
      "199, 3, 0.53270, 0.32493, 0.32565\n",
      "199, 4, 0.53422, 0.32493, 0.32565\n",
      "199, 0.32561\n",
      "199, 0.92009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(shuffle=True, n_splits=n_folds, random_state=1)\n",
    "\n",
    "top_count = 5\n",
    "def cross_val_predict_with_test(results):\n",
    "    predictions = []\n",
    "    predictions_train = np.zeros((train.shape[0], 200, top_count))\n",
    "    predictions_test = np.zeros((test.shape[0], 200, top_count))\n",
    "    for i in tqdm.tqdm_notebook(range(200)):\n",
    "        f = 'var_%d' % i\n",
    "        f_vc = '%s_vc' % f\n",
    "        runs = sorted(results[i], key=lambda x: x[1])\n",
    "        X_i = train[[f, f_vc]].values\n",
    "        y_i = train['target'].values\n",
    "        X_test_i = test[[f, f_vc]].values\n",
    "        \n",
    "        p_folds = []\n",
    "        for j in range(top_count): \n",
    "            s, s2, parameters, _ = runs[j]\n",
    "            fixed_parameters = dict(parameters)\n",
    "            \n",
    "            model = LGBMClassifier(**fixed_parameters)\n",
    "            for k, (train_i, val_i) in enumerate(cv.split(X_i, y_i)):\n",
    "                model.fit(X_i[train_i], y_i[train_i])\n",
    "                p_train_k = model.predict_proba(X_i[val_i])[:, 1]\n",
    "                predictions_train[val_i, i, j] = p_train_k\n",
    "                \n",
    "                p_test_k = model.predict_proba(X_test_i)[:, 1]\n",
    "                predictions_test[:, i, j] += p_test_k / n_folds\n",
    "                \n",
    "            p = predictions_train[:, i, j]\n",
    "            print '%d, %d, %.5f, %.5f, %.5f' % (i, j, s, s2, log_loss(train['target'], p))\n",
    "\n",
    "        p = np.mean(predictions_train[:, i, :], axis=1)\n",
    "        print '%d, %.5f' % (i, log_loss(train['target'], p))\n",
    "        predictions.append(p)\n",
    "        print '%d, %.5f' % (i, roc_auc_score(train['target'], aggregate(np.column_stack(predictions))))\n",
    "        print\n",
    "        \n",
    "    return predictions_train, predictions_test, predictions\n",
    "\n",
    "predictions_train, predictions_test, predictions = cross_val_predict_with_test(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = np.column_stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50, 0.92010\n",
      "0.51, 0.92010\n",
      "0.52, 0.92010\n",
      "0.53, 0.92010\n",
      "0.54, 0.92010\n",
      "0.55, 0.92011\n",
      "0.56, 0.92011\n",
      "0.57, 0.92011\n",
      "0.58, 0.92011\n",
      "0.59, 0.92011\n",
      "0.60, 0.92012\n",
      "0.61, 0.92012\n",
      "0.62, 0.92012\n",
      "0.63, 0.92012\n",
      "0.64, 0.92013\n",
      "0.65, 0.92013\n",
      "0.66, 0.92013\n",
      "0.67, 0.92014\n",
      "0.68, 0.92014\n",
      "0.69, 0.92014\n",
      "0.70, 0.92014\n",
      "0.71, 0.92014\n",
      "0.72, 0.92014\n",
      "0.73, 0.92014\n",
      "0.74, 0.92014\n",
      "0.75, 0.92015\n",
      "0.76, 0.92015\n",
      "0.77, 0.92015\n",
      "0.78, 0.92016\n",
      "0.79, 0.92017\n",
      "0.80, 0.92017\n",
      "0.81, 0.92018\n",
      "0.82, 0.92018\n",
      "0.83, 0.92018\n",
      "0.84, 0.92019\n",
      "0.85, 0.92018\n",
      "0.86, 0.92015\n",
      "0.87, 0.92011\n",
      "0.88, 0.92001\n",
      "0.89, 0.91984\n",
      "0.90, 0.91963\n",
      "0.91, 0.91935\n",
      "0.92, 0.91898\n",
      "0.93, 0.91852\n",
      "0.94, 0.91796\n",
      "0.95, 0.91731\n",
      "0.96, 0.91649\n",
      "0.97, 0.91553\n",
      "0.98, 0.91444\n",
      "0.99, 0.91328\n",
      "1.00, 0.91209\n"
     ]
    }
   ],
   "source": [
    "def aggregate(p, low_threshold = 0.0, high_threshold = 100.0):\n",
    "    p = p.copy()\n",
    "    p *= 10\n",
    "    p[p < low_threshold] = low_threshold\n",
    "    p[p > high_threshold] = high_threshold\n",
    "    \n",
    "    return np.prod(p, axis=1)\n",
    "\n",
    "for low_threshold in np.linspace(0.5, 1, 51):\n",
    "    x = aggregate(p_train.copy(), low_threshold)\n",
    "    print '%.2f, %.5f' % (low_threshold, roc_auc_score(train['target'], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 200, 5), (200000, 200, 5))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train.shape, predictions_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['individual_models_min_0.84_cv_0.91969.dump']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump((predictions_train, predictions_test), 'individual_models_min_0.84_cv_0.91969.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = aggregate(predictions_train.mean(axis=2), low_threshold=0.84)\n",
    "p_test = aggregate(predictions_test.mean(axis=2), low_threshold=0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00, 0.92019\n"
     ]
    }
   ],
   "source": [
    "print '%.2f, %.5f' % (low_threshold, roc_auc_score(train['target'], p_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 morph morph 5.3K Apr  1 19:02 closest_features.dump\r\n",
      "-rw-rw-r-- 1 morph morph 611M Apr  4 18:13 individual_models_min_0.84_cv_0.91969_averaged.dump\r\n",
      "-rw-rw-r-- 1 morph morph 9.0G Apr  4 18:10 individual_models_min_0.84_cv_0.91969.dump\r\n",
      "-rw-rw-r-- 1 morph morph 3.1M Apr  4 23:27 individual_models_threshold_0.84_cv_0.92019_10fold_final.dump\r\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "_ = joblib.dump((p_train, p_test), 'individual_models_threshold_0.84_cv_0.92019_10fold_final_train.dump')\n",
    "!ls -lh *.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('kaggle-santander-ctp-2019/predictions/train/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy', p_train)\n",
    "np.save('kaggle-santander-ctp-2019/predictions/test/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy', p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ID_code', u'target', u'var_0', u'var_1', u'var_2', u'var_3', u'var_4',\n",
       "       u'var_5', u'var_6', u'var_7',\n",
       "       ...\n",
       "       u'var_190_vc', u'var_191_vc', u'var_192_vc', u'var_193_vc',\n",
       "       u'var_194_vc', u'var_195_vc', u'var_196_vc', u'var_197_vc',\n",
       "       u'var_198_vc', u'var_199_vc'],\n",
       "      dtype='object', length=402)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof = train[['ID_code']].copy()\n",
    "train_oof['target'] = p_train\n",
    "test_oof = test[['ID_code']].copy()\n",
    "test_oof['target'] = p_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '9202_separate_feature_models_threshold_0.84_10fold_04042353.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_name = os.path.join('kaggle-santander-ctp-2019/predictions/train', name)\n",
    "test_name = os.path.join('kaggle-santander-ctp-2019/predictions/test', name)\n",
    "train_oof.to_csv(train_name, index=False)\n",
    "test_oof.to_csv(test_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstaged changes after reset:\r\n",
      "D\tpredictions/test/9202_separate_feature_models_threshold_0.84_10fold_04042353.csv\r\n",
      "D\tpredictions/train/9202_separate_feature_models_threshold_0.84_10fold_04042353.csv\r\n"
     ]
    }
   ],
   "source": [
    "!cd kaggle-santander-ctp-2019; git reset HEAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm 'predictions/test/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy'\r\n",
      "rm 'predictions/train/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy'\r\n"
     ]
    }
   ],
   "source": [
    "!cd kaggle-santander-ctp-2019; git rm predictions/train/{name} predictions/test/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd kaggle-santander-ctp-2019; git add predictions/train/{name} predictions/test/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master d9eb0e7] Remove npy\r\n",
      " 2 files changed, 0 insertions(+), 0 deletions(-)\r\n",
      " delete mode 100644 predictions/test/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy\r\n",
      " delete mode 100644 predictions/train/9202_separate_feature_models_threshold_0.84_10fold_04042353.npy\r\n"
     ]
    }
   ],
   "source": [
    "!cd kaggle-santander-ctp-2019; git commit predictions/train/{name} predictions/test/{name} -m \"Remove npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting objects: 5, done.\n",
      "Delta compression using up to 8 threads.\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 396 bytes | 0 bytes/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/sergeyshilin/kaggle-santander-ctp-2019.git\n",
      "   5e92a85..d9eb0e7  master -> master\n"
     ]
    }
   ],
   "source": [
    "!cd kaggle-santander-ctp-2019; git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['individual_models_min_0.84_cv_0.91969_averaged.dump']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump((predictions_train.mean(axis=2), predictions_test.mean(axis=3).mean(axis=2)), 'individual_models_min_0.84_cv_0.91969_averaged.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 morph morph 611M Apr  4 18:13 individual_models_min_0.84_cv_0.91969_averaged.dump\r\n",
      "-rw-rw-r-- 1 morph morph 9.0G Apr  4 18:10 individual_models_min_0.84_cv_0.91969.dump\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh individual_models_min_0.84_cv_0.91969*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_p(results, f):\n",
    "    predictions = []\n",
    "    scores = []\n",
    "    scores2 = []\n",
    "    for i in range(200):\n",
    "        _, _, _, p = f(results[i])\n",
    "        predictions.append(p)\n",
    "        \n",
    "        score = roc_auc_score(b['target'], p)\n",
    "        score2 = log_loss(b['target'], p)\n",
    "        scores.append(score)\n",
    "        scores2.append(score2)\n",
    "        \n",
    "    print '%.5f, %.5f' % (np.mean(scores), np.mean(scores2))\n",
    "    predictions = np.column_stack(predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52610, 0.32485\n",
      "0.92289\n",
      "0.52610, 0.32485\n",
      "0.92243\n",
      "0.52781, 0.32494\n",
      "0.91368\n",
      "0.52228, 0.32512\n",
      "0.89332\n"
     ]
    }
   ],
   "source": [
    "def min_log_loss(runs):\n",
    "    return min(runs, key=lambda x: x[1])\n",
    "\n",
    "def min_log_loss_n(runs, n=3):\n",
    "    runs = sorted(runs, key=lambda x: x[1])\n",
    "    predictions = np.column_stack([p for _, _, _, p in runs[:n]])\n",
    "    \n",
    "    return None, None, None, np.mean(predictions, axis=1)\n",
    "\n",
    "for f in [\n",
    "    min_log_loss,\n",
    "    min_log_loss_n,\n",
    "    max,\n",
    "    random.choice,\n",
    "]:\n",
    "    p = select_p(results, f)\n",
    "    p = aggregate(p)\n",
    "    t = b['target'].values\n",
    "    \n",
    "    print '%.5f' % roc_auc_score(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves\n",
      "[(11, 114), (23, 23), (5, 21), (29, 16), (41, 12)]\n",
      "subsample_freq\n",
      "[(2, 105), (5, 35), (8, 14), (1, 11), (9, 11)]\n",
      "learning_rate\n",
      "[(0.033, 114), (0.09000000000000001, 23), (0.027, 12), (0.032, 11), (0.057999999999999996, 8)]\n",
      "n_estimators\n",
      "[(100, 135), (300, 25), (200, 16), (400, 14), (500, 8)]\n",
      "subsample\n",
      "[(0.5, 126), (0.8999999999999999, 33), (0.6, 15), (0.4, 11), (0.7999999999999999, 8)]\n",
      "random_state\n",
      "[(0, 200)]\n",
      "max_depth\n",
      "[(2, 200)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "best_parameters = [\n",
    "    min_log_loss(results[i])[2] \n",
    "    for i in range(200)\n",
    "]\n",
    "\n",
    "keys = best_parameters[0].keys()\n",
    "for k in keys:\n",
    "    print k\n",
    "    print Counter([p[k] for p in best_parameters]).most_common(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv.zip')\n",
    "submission['target'] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f98981593d0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "submission['target'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2.28M/2.28M [00:02<00:00, 800kB/s]\n",
      "Successfully submitted to Santander Customer Transaction Prediction"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c santander-customer-transaction-prediction -f submission.csv.gz -m \"Individual feature predictions mix: local 1-fold validation 0.92303 (can be overfit)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
